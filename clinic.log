ðŸš€ Starting Enhanced Pipeline - Manual + Gap-Based AI Test Flow
===================================================================

ðŸŽ¯ Target Directory: /home/sigmoid/test-repos/clinic

ðŸ§¹ Cleaning previous coverage data...
âœ… Coverage data cleaned

ðŸ” Running detect_manual_tests.py on target repo...
ðŸ” Scanning repository for manual test directories in: /home/sigmoid/test-repos/clinic

ðŸ“Š Detection Result:
{
  "manual_tests_found": true,
  "manual_test_paths": [
    "/home/sigmoid/test-repos/clinic/tests"
  ],
  "test_files_count": 5,
  "test_dirs_detail": {
    "/home/sigmoid/test-repos/clinic/tests": [
      "/home/sigmoid/test-repos/clinic/tests/test_middleware.py",
      "/home/sigmoid/test-repos/clinic/tests/test_api.py",
      "/home/sigmoid/test-repos/clinic/tests/conftest.py",
      "/home/sigmoid/test-repos/clinic/tests/test_model.py",
      "/home/sigmoid/test-repos/clinic/tests/test_auth.py"
    ]
  }
}

ðŸ“ Manual Tests Found: True
ðŸ“‚ Test Paths: /home/sigmoid/test-repos/clinic/tests

âœ… Manual test cases detected. Running pytest with coverage analysis...

ðŸ“¦ Installing project dependencies...
Requirement already satisfied: fastapi==0.104.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (0.104.1)
Requirement already satisfied: uvicorn==0.24.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.24.0)
Requirement already satisfied: pydantic==2.5.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 4)) (2.5.0)
Requirement already satisfied: torch==2.4.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (2.4.0)
Requirement already satisfied: transformers==4.35.2 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (4.35.2)
Requirement already satisfied: tokenizers==0.15.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 9)) (0.15.0)
Requirement already satisfied: accelerate==0.24.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 10)) (0.24.1)
Requirement already satisfied: python-jose==3.3.0 in ./venv/lib/python3.12/site-packages (from python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (3.3.0)
Requirement already satisfied: python-multipart==0.0.6 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 14)) (0.0.6)
Requirement already satisfied: prometheus-client==0.19.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 17)) (0.19.0)
Requirement already satisfied: aiofiles==23.2.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 20)) (23.2.1)
Requirement already satisfied: httpx==0.25.2 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (0.25.2)
Requirement already satisfied: psutil==5.9.6 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 24)) (5.9.6)
Requirement already satisfied: pytest==7.4.3 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 27)) (7.4.3)
Requirement already satisfied: pytest-asyncio==0.21.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 28)) (0.21.1)
Requirement already satisfied: pytest-cov==4.1.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 29)) (4.1.0)
Requirement already satisfied: black==23.10.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (23.10.1)
Requirement already satisfied: isort==5.12.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 31)) (5.12.0)
Requirement already satisfied: flake8==6.1.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (6.1.0)
Requirement already satisfied: mypy==1.7.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 33)) (1.7.0)
Requirement already satisfied: bandit==1.7.5 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (1.7.5)
Requirement already satisfied: safety==2.3.4 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (2.3.4)
Requirement already satisfied: python-dotenv==1.0.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 38)) (1.0.0)
Requirement already satisfied: anyio<4.0.0,>=3.7.1 in ./venv/lib/python3.12/site-packages (from fastapi==0.104.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (3.7.1)
Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./venv/lib/python3.12/site-packages (from fastapi==0.104.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (0.27.0)
Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from fastapi==0.104.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (4.15.0)
Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic==2.5.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 4)) (0.7.0)
Requirement already satisfied: pydantic-core==2.14.1 in ./venv/lib/python3.12/site-packages (from pydantic==2.5.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 4)) (2.14.1)
Requirement already satisfied: click>=7.0 in ./venv/lib/python3.12/site-packages (from uvicorn==0.24.0->uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (8.3.0)
Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.12/site-packages (from uvicorn==0.24.0->uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.16.0)
Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.20.0)
Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (1.14.0)
Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.5)
Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.1.6)
Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (2025.10.0)
Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (80.9.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (2.20.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: triton==3.0.0 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.0.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (0.36.0)
Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2.3.4)
Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2025.11.3)
Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2.32.5)
Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (4.67.1)
Requirement already satisfied: ecdsa!=0.15 in ./venv/lib/python3.12/site-packages (from python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (0.19.1)
Requirement already satisfied: rsa in ./venv/lib/python3.12/site-packages (from python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (4.9.1)
Requirement already satisfied: pyasn1 in ./venv/lib/python3.12/site-packages (from python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (0.6.1)
Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (2025.10.5)
Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (1.0.9)
Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (3.11)
Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (1.3.1)
Requirement already satisfied: iniconfig in ./venv/lib/python3.12/site-packages (from pytest==7.4.3->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 27)) (2.3.0)
Requirement already satisfied: pluggy<2.0,>=0.12 in ./venv/lib/python3.12/site-packages (from pytest==7.4.3->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 27)) (1.6.0)
Requirement already satisfied: coverage>=5.2.1 in ./venv/lib/python3.12/site-packages (from coverage[toml]>=5.2.1->pytest-cov==4.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 29)) (7.11.3)
Requirement already satisfied: mypy-extensions>=0.4.3 in ./venv/lib/python3.12/site-packages (from black==23.10.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (1.1.0)
Requirement already satisfied: pathspec>=0.9.0 in ./venv/lib/python3.12/site-packages (from black==23.10.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (0.12.1)
Requirement already satisfied: platformdirs>=2 in ./venv/lib/python3.12/site-packages (from black==23.10.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (4.5.0)
Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from flake8==6.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (0.7.0)
Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in ./venv/lib/python3.12/site-packages (from flake8==6.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (2.11.1)
Requirement already satisfied: pyflakes<3.2.0,>=3.1.0 in ./venv/lib/python3.12/site-packages (from flake8==6.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (3.1.0)
Requirement already satisfied: GitPython>=1.0.1 in ./venv/lib/python3.12/site-packages (from bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (3.1.45)
Requirement already satisfied: stevedore>=1.20.0 in ./venv/lib/python3.12/site-packages (from bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (5.5.0)
Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (14.2.0)
Requirement already satisfied: dparse>=0.6.2 in ./venv/lib/python3.12/site-packages (from safety==2.3.4->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (0.6.4)
Requirement already satisfied: ruamel.yaml>=0.17.21 in ./venv/lib/python3.12/site-packages (from safety==2.3.4->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (0.18.16)
Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.9.86)
Requirement already satisfied: cryptography>=3.4.0 in ./venv/lib/python3.12/site-packages (from python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (46.0.3)
Requirement already satisfied: httptools>=0.5.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.7.1)
Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.22.1)
Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (1.1.1)
Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (15.0.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (1.2.0)
Requirement already satisfied: cffi>=2.0.0 in ./venv/lib/python3.12/site-packages (from cryptography>=3.4.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (2.0.0)
Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (2.23)
Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.12/site-packages (from ecdsa!=0.15->python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (1.10.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.12/site-packages (from GitPython>=1.0.1->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (4.0.12)
Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.1->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (5.0.2)
Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in ./venv/lib/python3.12/site-packages (from ruamel.yaml>=0.17.21->safety==2.3.4->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (0.2.14)
Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.0.3)
Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (3.4.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2.5.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (4.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (2.19.2)
Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (0.1.2)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (1.3.0)

ðŸ“‚ Copying manual tests to local folder: ./tests/manual

âœ… Manual test files copied to ./tests/manual
ðŸ“„ Files:
./tests/manual/test_middleware.py
./tests/manual/test_api.py
./tests/manual/test_model.py
./tests/manual/test_auth.py

ðŸ§ª Running manual tests from local directory with coverage analysis: ./tests/manual

============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.3, pluggy-1.6.0 -- /home/sigmoid/TECH_DEMO/new-tech-demo/venv/bin/python3
cachedir: .pytest_cache
metadata: {'Python': '3.12.3', 'Platform': 'Linux-6.14.0-33-generic-x86_64-with-glibc2.39', 'Packages': {'pytest': '7.4.3', 'pluggy': '1.6.0'}, 'Plugins': {'html': '4.1.1', 'django': '4.11.1', 'asyncio': '0.21.1', 'mock': '3.15.1', 'anyio': '3.7.1', 'metadata': '3.1.1', 'cov': '4.1.0'}}
rootdir: /home/sigmoid/TECH_DEMO/new-tech-demo
configfile: pytest.ini
plugins: html-4.1.1, django-4.11.1, asyncio-0.21.1, mock-3.15.1, anyio-3.7.1, metadata-3.1.1, cov-4.1.0
asyncio: mode=Mode.AUTO
collecting ... collected 88 items

tests/manual/test_api.py::TestHealthEndpoint::test_health_check_success PASSED [  1%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_success PASSED [  2%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_empty_sentence PASSED [  3%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[The patient denies chest pain.] PASSED [  4%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[He has a history of hypertension.] PASSED [  5%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[No signs of pneumonia were observed.] PASSED [  6%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_success PASSED [  7%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_empty_list PASSED [  9%]
tests/manual/test_api.py::TestRootEndpoint::test_root_endpoint PASSED    [ 10%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_conditional PASSED [ 11%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_uncertainty PASSED [ 12%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_no_rule PASSED [ 13%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_multiple_sentences PASSED [ 14%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_positive PASSED [ 15%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_negative PASSED [ 17%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_positive PASSED [ 18%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_negative PASSED [ 19%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_predict_with_hybrid_pipeline PASSED [ 20%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_batch_predict_with_hybrid_pipeline PASSED [ 21%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_no_api_keys PASSED  [ 22%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_api_keys PASSED [ 23%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_whitespace_api_keys PASSED [ 25%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_no_keys_required PASSED [ 26%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_valid PASSED  [ 27%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_invalid PASSED [ 28%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint FAILED [ 29%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_root_endpoint PASSED [ 30%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_metrics_endpoint PASSED [ 31%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required FAILED [ 32%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request FAILED [ 34%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid FAILED [ 35%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer FAILED [ 36%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header FAILED [ 37%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param FAILED [ 38%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key FAILED [ 39%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_multiple_keys PASSED [ 40%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_empty_key_string PASSED [ 42%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_whitespace_only PASSED [ 43%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_special_characters PASSED [ 44%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_case_sensitive PASSED [ 45%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init PASSED [ 46%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init_with_csp PASSED [ 47%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_no_csp_http PASSED [ 48%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_with_csp_https PASSED [ 50%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_init PASSED [ 51%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 52%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_client PASSED [ 53%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_unknown PASSED [ 54%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_health_endpoint PASSED [ 55%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_under_limit PASSED [ 56%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_over_limit PASSED [ 57%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_init PASSED [ 59%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 60%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_client PASSED [ 61%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_unknown PASSED [ 62%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_success PASSED [ 63%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception FAILED [ 64%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_init PASSED [ 65%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_success PASSED [ 67%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_error PASSED [ 68%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_exception PASSED [ 69%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_init PASSED [ 70%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_not_loaded PASSED [ 71%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_after_init PASSED [ 72%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_no_model PASSED [ 73%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_available PASSED [ 75%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_not_available PASSED [ 76%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success FAILED [ 77%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_tokenizer_failure PASSED [ 78%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_model_failure PASSED [ 79%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_not_loaded PASSED [ 80%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_not_loaded PASSED [ 81%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success FAILED [ 82%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_pipeline_error PASSED [ 84%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_success PASSED [ 85%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_list_result PASSED [ 86%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_nested_list_result PASSED [ 87%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unexpected_result_type PASSED [ 88%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success FAILED [ 89%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_success PASSED [ 90%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_single_result PASSED [ 92%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info PASSED [ 93%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info_cuda_available PASSED [ 94%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_empty_result PASSED [ 95%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_none_result PASSED [ 96%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_empty_results PASSED [ 97%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_mixed_result_types PASSED [ 98%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unknown_label PASSED [100%]

=================================== FAILURES ===================================
_____________ TestVerifyAPIKey.test_verify_api_key_health_endpoint _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a4828170>
auth_client = <starlette.testclient.TestClient object at 0x7aa1da19b260>

    def test_verify_api_key_health_endpoint(self, auth_client):
        """Test that health endpoint doesn't require API key"""
        response = auth_client.get("/health")
>       assert response.status_code == 200
E       assert 503 == 200
E        +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:55: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.main:main.py:229 Health check failed:
____________ TestVerifyAPIKey.test_verify_api_key_no_keys_required _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a4828b90>
auth_client = <starlette.testclient.TestClient object at 0x7aa1da19dc70>

    def test_verify_api_key_no_keys_required(self, auth_client):
        """Test endpoints when no API keys are required"""
        with patch.dict(os.environ, {"API_KEYS": "", "REQUIRE_API_KEY": "false"}):
            response = auth_client.get("/model/info")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:71: AssertionError
__________ TestVerifyAPIKey.test_verify_api_key_missing_from_request ___________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a4829040>
auth_client = <starlette.testclient.TestClient object at 0x7aa1da19c350>

    def test_verify_api_key_missing_from_request(self, auth_client):
        """Test API key verification when key is missing"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info")
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:79: AssertionError
_________________ TestVerifyAPIKey.test_verify_api_key_invalid _________________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a48293a0>
auth_client = <starlette.testclient.TestClient object at 0x7aa1da380650>

    def test_verify_api_key_invalid(self, auth_client):
        """Test API key verification with invalid key"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer invalid_key"}
            )
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:91: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_bearer _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a4829700>
auth_client = <starlette.testclient.TestClient object at 0x7aa1d9f0f4d0>

    def test_verify_api_key_valid_bearer(self, auth_client):
        """Test API key verification with valid Bearer token"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer test_key"}
            )
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:103: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_header _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a4829a60>
auth_client = <starlette.testclient.TestClient object at 0x7aa1d9f14200>

    def test_verify_api_key_valid_header(self, auth_client):
        """Test API key verification with valid X-API-Key header"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info", headers={"X-API-Key": "test_key"})
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:111: AssertionError
____________ TestVerifyAPIKey.test_verify_api_key_valid_query_param ____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a4823e00>
auth_client = <starlette.testclient.TestClient object at 0x7aa1da146990>

    def test_verify_api_key_valid_query_param(self, auth_client):
        """Test API key verification with valid query parameter"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info?api_key=test_key")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:119: AssertionError
___________ TestVerifyAPIKey.test_verify_api_key_logging_invalid_key ___________

self = <MagicMock name='logger.warning' id='134835564771232'>

    def assert_called(self):
        """assert that the mock was called at least once
        """
        if self.call_count == 0:
            msg = ("Expected '%s' to have been called." %
                   (self._mock_name or 'mock'))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to have been called.

/usr/lib/python3.12/unittest/mock.py:913: AssertionError

During handling of the above exception, another exception occurred:

self = <manual.test_auth.TestVerifyAPIKey object at 0x7aa2a48236e0>

    def test_verify_api_key_logging_invalid_key(self):
        """Test that invalid API key attempts are logged"""
        # Standard library imports
        from unittest.mock import Mock
    
        from app.auth import verify_api_key
    
        with patch.dict(
            os.environ, {"API_KEYS": "valid_key", "REQUIRE_API_KEY": "true"}
        ), patch("app.auth.logger") as mock_logger:
            mock_request = Mock()
            mock_request.url.path = "/api/test"
            mock_request.client = Mock()
            mock_request.client.host = "192.168.1.1"
            mock_request.headers = {}
            mock_request.query_params = {}
    
            # This should raise an exception and log the invalid attempt
            try:
                verify_api_key(mock_request, None)
            except Exception:
                pass
    
            # Verify that warning was logged for invalid key attempt
>           mock_logger.warning.assert_called()
E           AssertionError: Expected 'warning' to have been called.

tests/manual/test_auth.py:145: AssertionError
__________ TestRequestLoggingMiddleware.test_dispatch_with_exception ___________

request = <Mock id='134835555910336'>

    async def failing_call_next(request):
>       raise Exception("Test error")
E       Exception: Test error

tests/manual/test_middleware.py:248: Exception

During handling of the above exception, another exception occurred:

self = <app.middleware.RequestLoggingMiddleware object at 0x7aa1d969c1d0>
request = <Mock id='134835555910336'>
call_next = <function TestRequestLoggingMiddleware.test_dispatch_with_exception.<locals>.failing_call_next at 0x7aa1d9470ea0>

    async def dispatch(
        self, request: Request, call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        start_time = time.time()
        request_id = request.headers.get(
            "X-Request-ID", f"req-{int(start_time * 1000)}"
        )
    
        logger.info(
            json.dumps(
                {
                    "event": "request_started",
                    "request_id": request_id,
                    "method": request.method,
                    "path": request.url.path,
                    "client_ip": self.get_client_id(request),
                    "timestamp": start_time,
                }
            )
        )
    
        try:
            response = await call_next(request)
    
            duration = time.time() - start_time
            logger.info(
                json.dumps(
                    {
                        "event": "request_completed",
                        "request_id": request_id,
                        "status_code": response.status_code,
                        "duration_ms": duration * 1000,
                        "timestamp": time.time(),
                    }
                )
            )
    
            response.headers["X-Request-ID"] = request_id
            return response
    
        except Exception as e:
            duration = time.time() - start_time
            logger.error(
                json.dumps(
                    {
                        "event": "request_failed",
                        "request_id": request_id,
                        "error": str(e),
                        "duration_ms": duration * 1000,
>                       "timestamp": time.time(),
                    }
                )
            )

../../test-repos/clinic/app/middleware.py:155: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1134: in __call__
    return self._mock_call(*args, **kwargs)
/usr/lib/python3.12/unittest/mock.py:1138: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='time' id='134835555910528'>, args = (), kwargs = {}
effect = <list_iterator object at 0x7aa1d9f19420>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
>               result = next(effect)
E               StopIteration

/usr/lib/python3.12/unittest/mock.py:1195: StopIteration

The above exception was the direct cause of the following exception:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x7aa2a454bb00>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x7aa1d969c1d0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
            with pytest.raises(Exception, match="Test error"):
>               await middleware.dispatch(mock_request, failing_call_next)
E               RuntimeError: coroutine raised StopIteration

tests/manual/test_middleware.py:254: RuntimeError

During handling of the above exception, another exception occurred:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x7aa2a454bb00>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x7aa1d969c1d0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
>           with pytest.raises(Exception, match="Test error"):
E           AssertionError: Regex pattern did not match.
E            Regex: 'Test error'
E            Input: 'coroutine raised StopIteration'

tests/manual/test_middleware.py:253: AssertionError
______________ TestClinicalAssertionModel.test_load_model_success ______________

self = <manual.test_model.TestClinicalAssertionModel object at 0x7aa1da5d8c20>
mock_cuda = <MagicMock name='is_available' id='134835555379792'>
mock_pipeline = <MagicMock name='TextClassificationPipeline' id='134835555371296'>
mock_model_class = <MagicMock name='from_pretrained' id='134835564754032'>
mock_tokenizer = <MagicMock name='from_pretrained' id='134835564754608'>
model = <app.model.ClinicalAssertionModel object at 0x7aa1d96185c0>

    @pytest.mark.asyncio
    @patch("app.model.AutoTokenizer.from_pretrained")
    @patch("app.model.AutoModelForSequenceClassification.from_pretrained")
    @patch("app.model.TextClassificationPipeline")
    @patch("torch.cuda.is_available")
    async def test_load_model_success(
        self, mock_cuda, mock_pipeline, mock_model_class, mock_tokenizer, model
    ):
        """Test successful model loading"""
        mock_cuda.return_value = False
    
        # Mock the model and tokenizer
        mock_model_instance = Mock()
        mock_model_class.return_value = mock_model_instance
        mock_tokenizer_instance = Mock()
        mock_tokenizer.return_value = mock_tokenizer_instance
        mock_pipeline_instance = Mock()
        mock_pipeline.return_value = mock_pipeline_instance
    
        await model.load_model()
    
        assert model.tokenizer == mock_tokenizer_instance
>       assert model.model == mock_model_instance
E       AssertionError: assert <Mock name='from_pretrained().to().to()' id='134835567454032'> == <Mock name='from_pretrained()' id='134835567444432'>
E        +  where <Mock name='from_pretrained().to().to()' id='134835567454032'> = <app.model.ClinicalAssertionModel object at 0x7aa1d96185c0>.model

tests/manual/test_model.py:82: AssertionError
_______________ TestClinicalAssertionModel.test_predict_success ________________

self = <manual.test_model.TestClinicalAssertionModel object at 0x7aa1da5df080>
mock_loop = <MagicMock name='get_event_loop' id='134835568078976'>
model = <app.model.ClinicalAssertionModel object at 0x7aa1da5bcb00>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_success(self, mock_loop, model):
        """Test successful prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock the pipeline result
        mock_result = {"label": "LABEL_0", "score": 0.95}
        model.pipeline.return_value = [mock_result]
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_result)
    
        result = await model.predict("test sentence")
    
>       assert result == {"label": "PRESENT", "score": 0.95}
E       AssertionError: assert {'label': 'LA...'score': 0.95} == {'label': 'PR...'score': 0.95}
E         Omitting 1 identical items, use -vv to show
E         Differing items:
E         {'label': 'LABEL_0'} != {'label': 'PRESENT'}
E         Full diff:
E         - {'label': 'PRESENT', 'score': 0.95}
E         ?            ^^ ^^^^
E         + {'label': 'LABEL_0', 'score': 0.95}
E         ?            ^^^ ^^^

tests/manual/test_model.py:145: AssertionError
____________ TestClinicalAssertionModel.test_predict_batch_success _____________

self = <manual.test_model.TestClinicalAssertionModel object at 0x7aa1da5e9730>
mock_loop = <MagicMock name='get_event_loop' id='134835564799584'>
model = <app.model.ClinicalAssertionModel object at 0x7aa1d9f17080>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_batch_success(self, mock_loop, model):
        """Test successful batch prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock batch results
        mock_results = [
            [{"label": "LABEL_0", "score": 0.95}],
            [{"label": "LABEL_1", "score": 0.87}],
        ]
        model.pipeline.return_value = mock_results
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_results)
    
        result = await model.predict_batch(["sentence 1", "sentence 2"])
    
        expected = [
            {"label": "PRESENT", "score": 0.95},
            {"label": "ABSENT", "score": 0.87},
        ]
>       assert result == expected
E       AssertionError: assert [[{'label': '...core': 0.87}]] == [{'label': 'P...score': 0.87}]
E         At index 0 diff: [{'label': 'LABEL_0', 'score': 0.95}] != {'label': 'PRESENT', 'score': 0.95}
E         Full diff:
E         - [{'label': 'PRESENT', 'score': 0.95}, {'label': 'ABSENT', 'score': 0.87}]
E         ?             ^^ ^^^^                                - ^^
E         + [[{'label': 'LABEL_0', 'score': 0.95}], [{'label': 'LABEL_1', 'score': 0.87}]]
E         ? +            ^^^ ^^^                 +  +           +   ^^^                  +

tests/manual/test_model.py:237: AssertionError
=============================== warnings summary ===============================
venv/lib/python3.12/site-packages/transformers/utils/generic.py:441
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

venv/lib/python3.12/site-packages/transformers/utils/generic.py:309
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
  /home/sigmoid/TECH_DEMO/new-tech-demo/tests/manual/test_auth.py:140: RuntimeWarning: coroutine 'verify_api_key' was never awaited
    verify_api_key(mock_request, None)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform linux, python 3.12.3-final-0 -----------
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           154     40     16      5    72%   73-100, 160, 176-178, 257, 277-278, 333-336, 358, 365, 432-437, 454-457, 536-542
/home/sigmoid/test-repos/clinic/app/middleware.py      92      1     14      0    99%   159
/home/sigmoid/test-repos/clinic/app/model.py           82      3     14      1    96%   45->48, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      4     10      4    90%   37, 86, 90, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49     10     12      3    79%   15-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 487     73     84     15    82%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml

=========================== short test summary info ============================
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
FAILED tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success
================== 12 failed, 76 passed, 3 warnings in 19.78s ==================
ðŸ“Š Coverage report generated
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           157     42     18      6    71%   73-100, 160, 176-178, 257, 277-278, 333-336, 358, 365, 432-437, 454-457, 536-542, 558-559
/home/sigmoid/test-repos/clinic/app/middleware.py      92      1     14      0    99%   159
/home/sigmoid/test-repos/clinic/app/model.py           82      3     14      1    96%   45->48, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      4     10      4    90%   37, 86, 90, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49     10     12      3    79%   15-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 490     75     86     16    82%

âœ… Pytest completed for manual tests.

âœ… Manual Test Coverage: 85.01%

ðŸ“Š Coverage Summary:
  app/__init__.py: 100.0%
  app/auth.py: 61.5%
  app/main.py: 74.0%
  app/middleware.py: 98.9%
  app/model.py: 96.3%
  app/schemas.py: 94.3%
  app/utils.py: 79.6%

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
COVERAGE ANALYSIS PHASE
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
hello
ðŸ” Analyzing coverage gaps...
ðŸ” Analyzing coverage gaps...
================================================================================
COVERAGE GAP ANALYSIS REPORT
================================================================================

Overall Coverage: 85.01%
Total Statements: 487
Covered Statements: 414
Missing Statements: 73
AI Generation Needed: YES

FILES WITH COVERAGE GAPS:
--------------------------------------------------------------------------------

ðŸ“ app/auth.py
   Coverage: 61.54%
   Missing Lines: 15
   Uncovered: 42, 47, 49-50, 52-53, 55-56, 58-59, 63-66, 70

ðŸ“ app/main.py
   Coverage: 74.03%
   Missing Lines: 40
   Uncovered: 73-75, 77, 79-80, 83-84, 86-87, 89-91, 93, 96-97, 99-100, 160, 176, 178, 257, 277-278, 333-336, 358, 365, 432-433, 436-437, 454-455, 457, 536, 540, 542

ðŸ“ app/middleware.py
   Coverage: 98.91%
   Missing Lines: 1
   Uncovered: 159

ðŸ“ app/model.py
   Coverage: 96.34%
   Missing Lines: 3
   Uncovered: 117-119

ðŸ“ app/schemas.py
   Coverage: 94.29%
   Missing Lines: 4
   Uncovered: 37, 86, 90, 94

ðŸ“ app/utils.py
   Coverage: 79.59%
   Missing Lines: 10
   Uncovered: 15-18, 20, 26-28, 39, 55



================================================================================
âœ… Coverage gap analysis saved to: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json

âš ï¸  Coverage is below 90% (85.01%)
ðŸ¤– AI test generation recommended for uncovered code

âš ï¸  Coverage is below 90%
ðŸ¤– Initiating Gap-Based AI Test Generation...

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
GAP-BASED AI TEST GENERATION (Using Full AI Workflow)
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80

Warning: postprocess import failed: cannot import name 'extract_python_only' from 'src.gen.postprocess' (/home/sigmoid/TECH_DEMO/new-tech-demo/src/gen/postprocess.py); using fallbacks
ðŸŽ¯ Gap-focused mode enabled via --coverage-mode argument
Found 14 Python files in target directory
Project structure: Universal compatibility enabled
UNIVERSAL analysis for ANY PROJECT STRUCTURE in: /home/sigmoid/test-repos/clinic
Analyzing 8 Python files in project...
[framework_manager] Detection error in django: 'str' object has no attribute 'get'
[framework_manager] Framework(s) detected: ['fastapi', 'universal'] -> selected: fastapi
UNIVERSAL ANALYSIS COMPLETE:
   Files analyzed: 8
   Functions: 42 (top-level)
   Nested Functions: 23
   Classes: 28
   Methods: 23
   Routes: 8
   FastAPI Routes: 10
   Properties: 0
   Async functions: 39
   Imports tracked: 104
   Django models: 0
   Serializers: 0
   Views/ViewSets: 0
   Forms: 0
   Admin: 0
   Project packages: 2
   Detected Framework: fastapi
Starting UNIVERSAL test generation with gap-focused coverage mode...
UNIVERSAL test generation for ANY PROJECT STRUCTURE...

ðŸŽ¯ GAP-FOCUSED MODE: Analyzing coverage gaps...

================================================================================
GAP-FOCUSED MODE ACTIVE
================================================================================
Filtering analysis to target only uncovered code...
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6

ðŸŽ¯ Filtering analysis to focus on coverage gaps...

ðŸ“Š Gap-Focused Analysis Results:
   Original Functions: 42 â†’ Uncovered: 0
   Original Classes: 28 â†’ Uncovered: 0
   Original Methods: 23 â†’ Uncovered: 0
   Original Routes: 8 â†’ Uncovered: 8
   Total Reduction: 100.0% (focusing only on gaps)

âœ… Gap-focused analysis prepared
   Targeting 73 uncovered statements
================================================================================

âœ… Gap analysis complete: Targeting 0 uncovered functions, 0 uncovered classes, 0 uncovered methods
âœ… Successfully wrote: tests/generated/conftest.py
Created universal conftest: tests/generated/conftest.py
TESTGEN_FORCE environment variable: 'true'
ðŸš€ Force generation enabled - will regenerate all tests
Found 8 source files for force generation:
  - app/__init__.py
  - app/auth.py
  - app/main.py
  - app/middleware.py
  - app/model.py
  - app/schemas.py
  - app/utils.py
  - scripts/health-check.py
ðŸ§¹ Force mode: Cleaning up all existing generated tests
Cleaned up 0 existing test files
UNIVERSAL TARGET INCLUSION:
   Functions: 4 (including 4 nested)
   Classes: 0
   Methods: 0
   Routes: 18
   Project Packages: 2
Using universal targeting - all targets included
Analyzing required packages...
   auth: Local module (skipped pip install)
   middleware: Local module (skipped pip install)
   model: Local module (skipped pip install)
   schemas: Local module (skipped pip install)
   utils: Local module (skipped pip install)
Inferred 9 external packages: fastapi, httpx, prometheus_client, psutil, pydantic, starlette, torch, transformers, uvicorn
Installing 9 external packages...
Installing packages: fastapi, httpx, prometheus_client, psutil, pydantic, starlette, torch, transformers, uvicorn
   fastapi
   httpx
   prometheus_client
   psutil
   pydantic
   starlette
   torch
   transformers
   uvicorn
Successfully installed 9 packages.
UNIVERSAL COVERAGE TARGETS:
   Functions: 4
   Classes: 0
   Methods: 0
   Routes: 18
   Total Targets: 22
   Expected Coverage: Maximum
   Project Structure: Universal compatibility
Generating 1 UNIT test files for universal compatibility...
Generating unit test 1/1 for 4 targets
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
   ðŸ“Š Added 1691 chars of gap-focused context to prompt
âš ï¸ Syntax error in generated code for tests/generated/test_unit_20251113_134428_01.py: expected an indented block after 'if' statement on line 25 (<unknown>, line 28)
âœ… Fixed syntax errors in tests/generated/test_unit_20251113_134428_01.py
âœ… Successfully wrote: tests/generated/test_unit_20251113_134428_01.py
  test_unit_20251113_134428_01.py - 4 targets
Generating 1 INTEG test files for universal compatibility...
Generating integ test 1/1 for 18 targets
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
   ðŸ“Š Added 1691 chars of gap-focused context to prompt
âš ï¸ Syntax error in generated code for tests/generated/test_integ_20251113_134428_01.py: expected an indented block after 'if' statement on line 25 (<unknown>, line 28)
âœ… Fixed syntax errors in tests/generated/test_integ_20251113_134428_01.py
âœ… Successfully wrote: tests/generated/test_integ_20251113_134428_01.py
  test_integ_20251113_134428_01.py - 18 targets
Generating 1 E2E test files for universal compatibility...
Generating e2e test 1/1 for 18 targets
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
   ðŸ“Š Added 1691 chars of gap-focused context to prompt
âš ï¸ Syntax error in generated code for tests/generated/test_e2e_20251113_134428_01.py: expected an indented block after 'if' statement on line 23 (<unknown>, line 26)
âœ… Fixed syntax errors in tests/generated/test_e2e_20251113_134428_01.py
âœ… Successfully wrote: tests/generated/test_e2e_20251113_134428_01.py
  test_e2e_20251113_134428_01.py - 18 targets
Updated test mapping for 8 source files
Generation completed: 3 test files for 8 source files
ðŸ“Š Updated test manifest: tests/generated/_manifest.json
UNIVERSAL GENERATION COMPLETE: 3 test files
Expected Coverage: Maximum with REAL IMPORTS
Universal Compatibility: ENABLED
Targets Covered: 22
Project Structure: 2 packages detected
UNIVERSAL TEST GENERATION SUCCESSFUL!
UNIVERSAL Results:
   Generated: 3 test files
   Coverage Mode: gap-focused
   Universal Compatibility: ENABLED
   Targets Covered: 101
   Project Structure: Universal handling enabled
Run UNIVERSAL Tests:
   Basic: python -m pytest ./tests/generated -v
   Coverage: python -m pytest ./tests/generated --cov=. --cov-report=html
   Universal: PYTHONPATH=/home/sigmoid/test-repos/clinic python -m pytest ./tests/generated
=== AI Test Generation Completed ===
ðŸ§© Total AI-generated test files: 3
./tests/generated/test_e2e_20251113_134428_01.py
./tests/generated/test_integ_20251113_134428_01.py
./tests/generated/test_unit_20251113_134428_01.py

âœ… Gap-based AI test generation completed successfully

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
RUNNING COMBINED TESTS (Manual + AI Generated)
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80

ðŸ“¦ Installing project dependencies...
ðŸ§ª Running combined test suite...
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.3, pluggy-1.6.0 -- /home/sigmoid/TECH_DEMO/new-tech-demo/venv/bin/python3
cachedir: .pytest_cache
django: version: 5.2.8
metadata: {'Python': '3.12.3', 'Platform': 'Linux-6.14.0-33-generic-x86_64-with-glibc2.39', 'Packages': {'pytest': '7.4.3', 'pluggy': '1.6.0'}, 'Plugins': {'html': '4.1.1', 'django': '4.11.1', 'asyncio': '0.21.1', 'mock': '3.15.1', 'anyio': '3.7.1', 'metadata': '3.1.1', 'cov': '4.1.0'}}
rootdir: /home/sigmoid/TECH_DEMO/new-tech-demo
configfile: pytest.ini
plugins: html-4.1.1, django-4.11.1, asyncio-0.21.1, mock-3.15.1, anyio-3.7.1, metadata-3.1.1, cov-4.1.0
asyncio: mode=Mode.AUTO
collecting ... collected 143 items

tests/generated/test_e2e_20251113_134428_01.py::test_service_status_and_contains_expected_keys ERROR [  0%]
tests/generated/test_e2e_20251113_134428_01.py::test_root_initializing_and_then_healthy_state ERROR [  1%]
tests/generated/test_e2e_20251113_134428_01.py::test_metrics_endpoint_returns_prometheus_content_type ERROR [  2%]
tests/generated/test_e2e_20251113_134428_01.py::test_health_check_when_model_missing_returns_503 ERROR [  2%]
tests/generated/test_e2e_20251113_134428_01.py::test_health_check_returns_healthy_when_model_loaded_and_system_metrics ERROR [  3%]
tests/generated/test_e2e_20251113_134428_01.py::test_model_info_responds_503_when_model_not_loaded ERROR [  4%]
tests/generated/test_e2e_20251113_134428_01.py::test_model_info_returns_info_when_loaded ERROR [  4%]
tests/generated/test_e2e_20251113_134428_01.py::test_predict_raises_503_when_model_not_loaded ERROR [  5%]
tests/generated/test_e2e_20251113_134428_01.py::test_predict_success_path_monkeypatched_pipeline_and_sanitizer ERROR [  6%]
tests/generated/test_e2e_20251113_134428_01.py::test_predict_handles_model_predict_exception_and_returns_500 ERROR [  6%]
tests/generated/test_e2e_20251113_134428_01.py::test_predict_batch_rejects_over_max_size ERROR [  7%]
tests/generated/test_e2e_20251113_134428_01.py::test_predict_batch_success ERROR [  8%]
tests/generated/test_e2e_20251113_134428_01.py::test_system_metrics_reflects_model_loaded_and_system_values ERROR [  9%]
tests/generated/test_e2e_20251113_134428_01.py::test_model_info_requires_api_key_and_rejects_missing ERROR [  9%]
tests/generated/test_e2e_20251113_134428_01.py::test_sanitize_and_apply_pipeline_edge_cases ERROR [ 10%]
tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentence_parametrized[normal sentence-True] ERROR [ 11%]
tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentence_parametrized[-False] ERROR [ 11%]
tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentence_parametrized[None-False] ERROR [ 12%]
tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentences_parametrized[values0-2] ERROR [ 13%]
tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentences_parametrized[values1-0] ERROR [ 13%]
tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentences_parametrized[None-0] ERROR [ 14%]
tests/generated/test_e2e_20251113_134428_01.py::test_http_exception_handler_produces_structured_json ERROR [ 15%]
tests/generated/test_e2e_20251113_134428_01.py::test_general_exception_handler_catches_unexpected ERROR [ 16%]
tests/generated/test_integ_20251113_134428_01.py::test_service_status_endpoint_returns_expected_structure PASSED [ 16%]
tests/generated/test_integ_20251113_134428_01.py::test_health_check_when_model_not_loaded_triggers_503 PASSED [ 17%]
tests/generated/test_integ_20251113_134428_01.py::test_health_check_when_model_loaded_returns_health_response PASSED [ 18%]
tests/generated/test_integ_20251113_134428_01.py::test_model_info_endpoint_when_model_not_loaded_returns_503 PASSED [ 18%]
tests/generated/test_integ_20251113_134428_01.py::test_model_info_endpoint_when_model_loaded_returns_info FAILED [ 19%]
tests/generated/test_integ_20251113_134428_01.py::test_predict_assertion_returns_503_when_model_unavailable PASSED [ 20%]
tests/generated/test_integ_20251113_134428_01.py::test_predict_assertion_success_and_metrics_applied PASSED [ 20%]
tests/generated/test_integ_20251113_134428_01.py::test_predict_batch_rejects_oversized_batch_and_accepts_valid_batch PASSED [ 21%]
tests/generated/test_integ_20251113_134428_01.py::test_system_metrics_reflects_model_loaded_flag_and_system_metrics_values PASSED [ 22%]
tests/generated/test_integ_20251113_134428_01.py::test_root_endpoint_shows_initializing_and_healthy_states PASSED [ 23%]
tests/generated/test_integ_20251113_134428_01.py::test_schema_validators_handle_various_inputs[validate_sentence-normal sentence-True] SKIPPED [ 23%]
tests/generated/test_integ_20251113_134428_01.py::test_schema_validators_handle_various_inputs[validate_sentence--False] SKIPPED [ 24%]
tests/generated/test_integ_20251113_134428_01.py::test_schema_validators_handle_various_inputs[validate_sentence-None-False] SKIPPED [ 25%]
tests/generated/test_integ_20251113_134428_01.py::test_schema_validators_handle_various_inputs[validate_sentences-value3-True] SKIPPED [ 25%]
tests/generated/test_integ_20251113_134428_01.py::test_schema_validators_handle_various_inputs[validate_sentences-value4-False] SKIPPED [ 26%]
tests/generated/test_integ_20251113_134428_01.py::test_predict_batch_integration_with_sanitization_and_pipeline_applied PASSED [ 27%]
tests/generated/test_integ_20251113_134428_01.py::test_general_exception_handler_increments_request_count_and_returns_500 FAILED [ 27%]
tests/generated/test_unit_20251113_134428_01.py::test_predictionrequest_sentence_validation[normal-Patient has fever.-True] PASSED [ 28%]
tests/generated/test_unit_20251113_134428_01.py::test_predictionrequest_sentence_validation[empty_string--False] PASSED [ 29%]
tests/generated/test_unit_20251113_134428_01.py::test_predictionrequest_sentence_validation[whitespace-   \t\n  -False] PASSED [ 30%]
tests/generated/test_unit_20251113_134428_01.py::test_predictionrequest_sentence_validation[none_value-None-False] PASSED [ 30%]
tests/generated/test_unit_20251113_134428_01.py::test_predictionrequest_sentence_validation[long_string-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-True] FAILED [ 31%]
tests/generated/test_unit_20251113_134428_01.py::test_batchpredictionrequest_sentences_validation[single_valid-sentences0-True] PASSED [ 32%]
tests/generated/test_unit_20251113_134428_01.py::test_batchpredictionrequest_sentences_validation[multiple_valid-sentences1-True] PASSED [ 32%]
tests/generated/test_unit_20251113_134428_01.py::test_batchpredictionrequest_sentences_validation[contains_none-sentences2-False] PASSED [ 33%]
tests/generated/test_unit_20251113_134428_01.py::test_batchpredictionrequest_sentences_validation[empty_list-sentences3-False] PASSED [ 34%]
tests/generated/test_unit_20251113_134428_01.py::test_batchpredictionrequest_sentences_validation[non_string-sentences4-False] PASSED [ 34%]
tests/generated/test_unit_20251113_134428_01.py::test_middleware_dispatch_paths[False-200] PASSED [ 35%]
tests/generated/test_unit_20251113_134428_01.py::test_middleware_dispatch_paths[True-None] PASSED [ 36%]
tests/generated/test_unit_20251113_134428_01.py::test_model_predict_batch_success_and_error_paths PASSED [ 37%]
tests/generated/test_unit_20251113_134428_01.py::test_validate_sentence_direct_if_exposed SKIPPED [ 37%]
tests/generated/test_unit_20251113_134428_01.py::test_validate_sentences_direct_if_exposed SKIPPED [ 38%]
tests/manual/test_api.py::TestHealthEndpoint::test_health_check_success PASSED [ 39%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_success PASSED [ 39%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_empty_sentence PASSED [ 40%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[The patient denies chest pain.] PASSED [ 41%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[He has a history of hypertension.] PASSED [ 41%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[No signs of pneumonia were observed.] PASSED [ 42%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_success PASSED [ 43%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_empty_list PASSED [ 44%]
tests/manual/test_api.py::TestRootEndpoint::test_root_endpoint PASSED    [ 44%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_conditional PASSED [ 45%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_uncertainty PASSED [ 46%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_no_rule PASSED [ 46%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_multiple_sentences PASSED [ 47%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_positive PASSED [ 48%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_negative PASSED [ 48%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_positive PASSED [ 49%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_negative PASSED [ 50%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_predict_with_hybrid_pipeline PASSED [ 51%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_batch_predict_with_hybrid_pipeline PASSED [ 51%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_no_api_keys PASSED  [ 52%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_api_keys PASSED [ 53%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_whitespace_api_keys PASSED [ 53%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_no_keys_required PASSED [ 54%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_valid PASSED  [ 55%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_invalid PASSED [ 55%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint FAILED [ 56%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_root_endpoint PASSED [ 57%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_metrics_endpoint PASSED [ 58%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required FAILED [ 58%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request FAILED [ 59%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid FAILED [ 60%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer FAILED [ 60%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header FAILED [ 61%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param FAILED [ 62%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key FAILED [ 62%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_multiple_keys PASSED [ 63%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_empty_key_string PASSED [ 64%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_whitespace_only PASSED [ 65%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_special_characters PASSED [ 65%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_case_sensitive PASSED [ 66%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init PASSED [ 67%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init_with_csp PASSED [ 67%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_no_csp_http PASSED [ 68%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_with_csp_https PASSED [ 69%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_init PASSED [ 69%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 70%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_client PASSED [ 71%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_unknown PASSED [ 72%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_health_endpoint PASSED [ 72%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_under_limit PASSED [ 73%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_over_limit PASSED [ 74%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_init PASSED [ 74%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 75%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_client PASSED [ 76%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_unknown PASSED [ 76%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_success PASSED [ 77%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception FAILED [ 78%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_init PASSED [ 79%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_success PASSED [ 79%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_error PASSED [ 80%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_exception PASSED [ 81%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_init PASSED [ 81%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_not_loaded PASSED [ 82%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_after_init PASSED [ 83%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_no_model PASSED [ 83%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_available PASSED [ 84%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_not_available PASSED [ 85%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success FAILED [ 86%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_tokenizer_failure FAILED [ 86%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_model_failure FAILED [ 87%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_not_loaded PASSED [ 88%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_not_loaded PASSED [ 88%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success FAILED [ 89%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_pipeline_error PASSED [ 90%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_success PASSED [ 90%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_list_result PASSED [ 91%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_nested_list_result PASSED [ 92%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unexpected_result_type PASSED [ 93%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success FAILED [ 93%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_success PASSED [ 94%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_single_result PASSED [ 95%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info PASSED [ 95%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info_cuda_available PASSED [ 96%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_empty_result PASSED [ 97%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_none_result PASSED [ 97%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_empty_results PASSED [ 98%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_mixed_result_types PASSED [ 99%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unknown_label PASSED [100%]

==================================== ERRORS ====================================
_______ ERROR at setup of test_service_status_and_contains_expected_keys _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b507a40>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
---------------------------- Captured stdout setup -----------------------------
Detected and imported: app
_______ ERROR at setup of test_root_initializing_and_then_healthy_state ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b521010>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
___ ERROR at setup of test_metrics_endpoint_returns_prometheus_content_type ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3f9670>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
______ ERROR at setup of test_health_check_when_model_missing_returns_503 ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b523920>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_ ERROR at setup of test_health_check_returns_healthy_when_model_loaded_and_system_metrics _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3dedb0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_____ ERROR at setup of test_model_info_responds_503_when_model_not_loaded _____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3ab530>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
__________ ERROR at setup of test_model_info_returns_info_when_loaded __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3a8890>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_______ ERROR at setup of test_predict_raises_503_when_model_not_loaded ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b521ee0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_ ERROR at setup of test_predict_success_path_monkeypatched_pipeline_and_sanitizer _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b523ad0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_ ERROR at setup of test_predict_handles_model_predict_exception_and_returns_500 _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b325b50>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
__________ ERROR at setup of test_predict_batch_rejects_over_max_size __________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b324ec0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_________________ ERROR at setup of test_predict_batch_success _________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3b01d0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_ ERROR at setup of test_system_metrics_reflects_model_loaded_and_system_values _

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3deba0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
____ ERROR at setup of test_model_info_requires_api_key_and_rejects_missing ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3c47a0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
________ ERROR at setup of test_sanitize_and_apply_pipeline_edge_cases _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3c7530>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_ ERROR at setup of test_validate_sentence_parametrized[normal sentence-True] __

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b135670>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
________ ERROR at setup of test_validate_sentence_parametrized[-False] _________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b12e6f0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
______ ERROR at setup of test_validate_sentence_parametrized[None-False] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b3f9a60>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
______ ERROR at setup of test_validate_sentences_parametrized[values0-2] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b12e1e0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
______ ERROR at setup of test_validate_sentences_parametrized[values1-0] _______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b5edcd0>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
________ ERROR at setup of test_validate_sentences_parametrized[None-0] ________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b31cc80>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
____ ERROR at setup of test_http_exception_handler_produces_structured_json ____

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b5eea20>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
_____ ERROR at setup of test_general_exception_handler_catches_unexpected ______

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7a533b349f70>

    @pytest.fixture(autouse=True)
    def ensure_clean_env(monkeypatch):
        # Ensure predictable environment defaults for tests
        monkeypatch.setenv("ENVIRONMENT", "test")
        # Default to not requiring API key unless test explicitly sets env var and reloads app
        monkeypatch.delenv("REQUIRE_API_KEY", raising=False)
        monkeypatch.delenv("API_KEY", raising=False)
        monkeypatch.delenv("ENABLE_RATE_LIMITING", raising=False)
        monkeypatch.delenv("MAX_BATCH_SIZE", raising=False)
        # Inject a simple dummy model module to avoid heavy dependencies
        inject_dummy_model_module()
        # Reload app.main to pick up dummy model and env defaults
>       app_main = reload_app_main()

tests/generated/test_e2e_20251113_134428_01.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/generated/test_e2e_20251113_134428_01.py:82: in reload_app_main
    importlib.reload(sys.modules["app.main"])
/usr/lib/python3.12/importlib/__init__.py:131: in reload
    _bootstrap._exec(spec, module)
<frozen importlib._bootstrap>:866: in _exec
    ???
<frozen importlib._bootstrap_external>:995: in exec_module
    ???
<frozen importlib._bootstrap>:488: in _call_with_frames_removed
    ???
../../test-repos/clinic/app/main.py:45: in <module>
    REQUEST_COUNT = Counter(
venv/lib/python3.12/site-packages/prometheus_client/metrics.py:155: in __init__
    registry.register(self)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <prometheus_client.registry.CollectorRegistry object at 0x7a533b6b7a70>
collector = prometheus_client.metrics.Counter(http_requests)

    def register(self, collector: Collector) -> None:
        """Add a collector to the registry."""
        with self._lock:
            names = self._get_names(collector)
            duplicates = set(self._names_to_collectors).intersection(names)
            if duplicates:
>               raise ValueError(
                    'Duplicated timeseries in CollectorRegistry: {}'.format(
                        duplicates))
E               ValueError: Duplicated timeseries in CollectorRegistry: {'http_requests', 'http_requests_created', 'http_requests_total'}

venv/lib/python3.12/site-packages/prometheus_client/registry.py:43: ValueError
=================================== FAILURES ===================================
___________ test_model_info_endpoint_when_model_loaded_returns_info ____________

self = MemoryObjectReceiveStream(_state=MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=0, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)

    async def receive(self) -> T_co:
        await checkpoint()
        try:
>           return self.receive_nowait()

venv/lib/python3.12/site-packages/anyio/streams/memory.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = MemoryObjectReceiveStream(_state=MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=0, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)

    def receive_nowait(self) -> T_co:
        """
        Receive the next item if it can be done without waiting.
    
        :return: the received item
        :raises ~anyio.ClosedResourceError: if this send stream has been closed
        :raises ~anyio.EndOfStream: if the buffer is empty and this stream has been
            closed from the sending end
        :raises ~anyio.WouldBlock: if there are no items in the buffer and no tasks
            waiting to send
    
        """
        if self._closed:
            raise ClosedResourceError
    
        if self._state.waiting_senders:
            # Get the item from the next sender
            send_event, item = self._state.waiting_senders.popitem(last=False)
            self._state.buffer.append(item)
            send_event.set()
    
        if self._state.buffer:
            return self._state.buffer.popleft()
        elif not self._state.open_send_channels:
            raise EndOfStream
    
>       raise WouldBlock
E       anyio.WouldBlock

venv/lib/python3.12/site-packages/anyio/streams/memory.py:93: WouldBlock

During handling of the above exception, another exception occurred:

request = <starlette.requests.Request object at 0x7a533b1d4500>

    async def call_next(request: Request) -> Response:
        app_exc: typing.Optional[Exception] = None
        send_stream, recv_stream = anyio.create_memory_object_stream()
    
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: typing.Callable[[], typing.Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(request.receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def close_recv_stream_on_response_sent() -> None:
            await response_sent.wait()
            recv_stream.close()
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            async with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(close_recv_stream_on_response_sent)
        task_group.start_soon(coro)
    
        try:
>           message = await recv_stream.receive()

venv/lib/python3.12/site-packages/starlette/middleware/base.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = MemoryObjectReceiveStream(_state=MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=0, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)

    async def receive(self) -> T_co:
        await checkpoint()
        try:
            return self.receive_nowait()
        except WouldBlock:
            # Add ourselves in the queue
            receive_event = Event()
            container: list[T_co] = []
            self._state.waiting_receivers[receive_event] = container
    
            try:
                await receive_event.wait()
            except get_cancelled_exc_class():
                # Ignore the immediate cancellation if we already received an item, so as not to
                # lose it
                if not container:
                    raise
            finally:
                self._state.waiting_receivers.pop(receive_event, None)
    
            if container:
                return container[0]
            else:
>               raise EndOfStream
E               anyio.EndOfStream

venv/lib/python3.12/site-packages/anyio/streams/memory.py:118: EndOfStream

During handling of the above exception, another exception occurred:

    @pytest.mark.asyncio
    async def test_model_info_endpoint_when_model_loaded_returns_info():
        """UNIVERSAL test for maximum coverage."""
        """
        model_info returns the model.get_model_info mapped to response model when model is loaded.
        This hits the branch that returns ModelInfoResponse.
        """
        stub = ClinicalAssertionModelStub(loaded=True, model_info={"name": "stub", "version": "1.2", "labels": ["A"]})
        with TempModel(stub):
            async with AsyncClient(app=main.app, base_url="http://test") as ac:
>               resp = await ac.get("/model/info")

tests/generated/test_integ_20251113_134428_01.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/httpx/_client.py:1757: in get
    return await self.request(
venv/lib/python3.12/site-packages/httpx/_client.py:1530: in request
    return await self.send(request, auth=auth, follow_redirects=follow_redirects)
venv/lib/python3.12/site-packages/httpx/_client.py:1617: in send
    response = await self._send_handling_auth(
venv/lib/python3.12/site-packages/httpx/_client.py:1645: in _send_handling_auth
    response = await self._send_handling_redirects(
venv/lib/python3.12/site-packages/httpx/_client.py:1682: in _send_handling_redirects
    response = await self._send_single_request(request)
venv/lib/python3.12/site-packages/httpx/_client.py:1719: in _send_single_request
    response = await transport.handle_async_request(request)
venv/lib/python3.12/site-packages/httpx/_transports/asgi.py:162: in handle_async_request
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/fastapi/applications.py:1106: in __call__
    await super().__call__(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/applications.py:122: in __call__
    await self.middleware_stack(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/errors.py:184: in __call__
    raise exc
venv/lib/python3.12/site-packages/starlette/middleware/errors.py:162: in __call__
    await self.app(scope, receive, _send)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:108: in __call__
    response = await self.dispatch_func(request, call_next)
../../test-repos/clinic/app/middleware.py:176: in dispatch
    response = await call_next(request)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:84: in call_next
    raise app_exc
venv/lib/python3.12/site-packages/starlette/middleware/base.py:70: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:108: in __call__
    response = await self.dispatch_func(request, call_next)
../../test-repos/clinic/app/middleware.py:128: in dispatch
    response = await call_next(request)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:84: in call_next
    raise app_exc
venv/lib/python3.12/site-packages/starlette/middleware/base.py:70: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
venv/lib/python3.12/site-packages/starlette/middleware/gzip.py:24: in __call__
    await responder(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/gzip.py:44: in __call__
    await self.app(scope, receive, self.send_with_gzip)
venv/lib/python3.12/site-packages/starlette/middleware/cors.py:83: in __call__
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/trustedhost.py:34: in __call__
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:108: in __call__
    response = await self.dispatch_func(request, call_next)
../../test-repos/clinic/app/middleware.py:30: in dispatch
    response = await call_next(request)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:84: in call_next
    raise app_exc
venv/lib/python3.12/site-packages/starlette/middleware/base.py:70: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py:79: in __call__
    raise exc
venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py:68: in __call__
    await self.app(scope, receive, sender)
venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py:20: in __call__
    raise e
venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py:17: in __call__
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/routing.py:718: in __call__
    await route.handle(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/routing.py:276: in handle
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/routing.py:66: in app
    response = await func(request)
venv/lib/python3.12/site-packages/fastapi/routing.py:274: in app
    raw_response = await run_endpoint_function(
venv/lib/python3.12/site-packages/fastapi/routing.py:191: in run_endpoint_function
    return await dependant.call(**values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @app.get(
        "/model/info",
        response_model=ModelInfoResponse,
        tags=["Model"],
        dependencies=[Depends(verify_api_key)] if os.getenv("REQUIRE_API_KEY") else [],
    )
    async def model_info() -> ModelInfoResponse:
        """Get detailed model information"""
        if not model or not model.is_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Model not loaded"
            )
    
>       return ModelInfoResponse(**model.get_model_info())
E       pydantic_core._pydantic_core.ValidationError: 4 validation errors for ModelInfoResponse
E       model_name
E         Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing
E       device
E         Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing
E       loaded
E         Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing
E       cuda_available
E         Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing

../../test-repos/clinic/app/main.py:257: ValidationError
------------------------------ Captured log call -------------------------------
ERROR    app.middleware:middleware.py:148 {"event": "request_failed", "request_id": "req-1763041762771", "error": "4 validation errors for ModelInfoResponse\nmodel_name\n  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\ndevice\n  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\nloaded\n  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\ncuda_available\n  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing", "duration_ms": 1.0976791381835938, "timestamp": 1763041762.7722163}
ERROR    app.main:main.py:540 Unhandled exception: 4 validation errors for ModelInfoResponse
model_name
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
device
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
loaded
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
cuda_available
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 98, in receive
    return self.receive_nowait()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 93, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 78, in call_next
    message = await recv_stream.receive()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 118, in receive
    raise EndOfStream
anyio.EndOfStream

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 176, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 128, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/gzip.py", line 24, in __call__
    await responder(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/gzip.py", line 44, in __call__
    await self.app(scope, receive, self.send_with_gzip)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/trustedhost.py", line 34, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 30, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 79, in __call__
    raise exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 68, in __call__
    await self.app(scope, receive, sender)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 20, in __call__
    raise e
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 17, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 718, in __call__
    await route.handle(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 276, in handle
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 66, in app
    response = await func(request)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/routing.py", line 274, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/routing.py", line 191, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/main.py", line 257, in model_info
    return ModelInfoResponse(**model.get_model_info())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/pydantic/main.py", line 164, in __init__
    __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)
pydantic_core._pydantic_core.ValidationError: 4 validation errors for ModelInfoResponse
model_name
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
device
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
loaded
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
cuda_available
  Field required [type=missing, input_value={'name': 'stub', 'version... '1.2', 'labels': ['A']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
___ test_general_exception_handler_increments_request_count_and_returns_500 ____

    @pytest.mark.asyncio
    async def test_general_exception_handler_increments_request_count_and_returns_500():
        """UNIVERSAL test for maximum coverage."""
        """
        Patch an endpoint dependency to raise a generic exception to exercise the general_exception_handler.
        We'll temporarily monkeypatch the root endpoint function to raise an exception and ensure the HTTP 500 handler runs.
        """
        orig_root = getattr(main, "root", None)
        async def broken_root():
            raise RuntimeError("unexpected failure")
        # Replace the route handler function in the FastAPI app routes list to broken_root
        # Find the route for path '/'
        route = None
        for r in list(main.app.routes):
            if getattr(r, "path", None) == "/":
                route = r
                break
        if route is None:
            # If route not found, skip
            pytest.skip("Root route not found to patch for exception handler test")
        # Monkeypatch the endpoint function by directly swapping in the handler used by the route
        # FastAPI stores endpoint function in route.endpoint
        orig_endpoint = getattr(route, "endpoint", None)
        route.endpoint = broken_root
        try:
            async with AsyncClient(app=main.app, base_url="http://test") as ac:
                resp = await ac.get("/")
>           assert resp.status_code == 500
E           assert 200 == 500
E            +  where 200 = <Response [200 OK]>.status_code

tests/generated/test_integ_20251113_134428_01.py:422: AssertionError
_ test_predictionrequest_sentence_validation[long_string-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-True] _

name = 'long_string'
sentence = 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
should_pass = True

    @pytest.mark.parametrize("name,sentence,should_pass", [
        ("normal", "Patient has fever.", True),
        ("empty_string", "", False),
        ("whitespace", "   \t\n  ", False),
        ("none_value", None, False),
        ("long_string", "a" * 10000, True),  # very long input as edge case
    ])
    def test_predictionrequest_sentence_validation(name, sentence, should_pass):
        """UNIVERSAL test for maximum coverage."""
        """
        Exercise the validate_sentence validator through the public PredictionRequest model.
        This will exercise validators defined for 'sentence' in app.schemas.
        """
        # Ensure the model exists
        if not hasattr(app_schemas, "PredictionRequest"):
            pytest.skip("PredictionRequest model not present in app.schemas")
        PR = getattr(app_schemas, "PredictionRequest")
    
        # Construct kwargs carefully: if sentence is None, still pass it to trigger validation
        kwargs = {"sentence": sentence}
        if should_pass:
>           inst = PR(**kwargs)
E           pydantic_core._pydantic_core.ValidationError: 1 validation error for PredictionRequest
E           sentence
E             String should have at most 1000 characters [type=string_too_long, input_value='aaaaaaaaaaaaaaaaaaaaaaaa...aaaaaaaaaaaaaaaaaaaaaaa', input_type=str]
E               For further information visit https://errors.pydantic.dev/2.5/v/string_too_long

tests/generated/test_unit_20251113_134428_01.py:76: ValidationError
_____________ TestVerifyAPIKey.test_verify_api_key_health_endpoint _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a540537a2d0>
auth_client = <starlette.testclient.TestClient object at 0x7a533ae3f680>

    def test_verify_api_key_health_endpoint(self, auth_client):
        """Test that health endpoint doesn't require API key"""
        response = auth_client.get("/health")
>       assert response.status_code == 200
E       assert 503 == 200
E        +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:55: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.main:main.py:229 Health check failed:
____________ TestVerifyAPIKey.test_verify_api_key_no_keys_required _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a5405390140>
auth_client = <starlette.testclient.TestClient object at 0x7a533b20de50>

    def test_verify_api_key_no_keys_required(self, auth_client):
        """Test endpoints when no API keys are required"""
        with patch.dict(os.environ, {"API_KEYS": "", "REQUIRE_API_KEY": "false"}):
            response = auth_client.get("/model/info")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:71: AssertionError
__________ TestVerifyAPIKey.test_verify_api_key_missing_from_request ___________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a54053904a0>
auth_client = <starlette.testclient.TestClient object at 0x7a533b31c500>

    def test_verify_api_key_missing_from_request(self, auth_client):
        """Test API key verification when key is missing"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info")
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:79: AssertionError
_________________ TestVerifyAPIKey.test_verify_api_key_invalid _________________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a5405390800>
auth_client = <starlette.testclient.TestClient object at 0x7a533b27d910>

    def test_verify_api_key_invalid(self, auth_client):
        """Test API key verification with invalid key"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer invalid_key"}
            )
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:91: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_bearer _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a5405390b60>
auth_client = <starlette.testclient.TestClient object at 0x7a533b1f57c0>

    def test_verify_api_key_valid_bearer(self, auth_client):
        """Test API key verification with valid Bearer token"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer test_key"}
            )
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:103: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_header _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a5405390ec0>
auth_client = <starlette.testclient.TestClient object at 0x7a533b1199a0>

    def test_verify_api_key_valid_header(self, auth_client):
        """Test API key verification with valid X-API-Key header"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info", headers={"X-API-Key": "test_key"})
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:111: AssertionError
____________ TestVerifyAPIKey.test_verify_api_key_valid_query_param ____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a5405391220>
auth_client = <starlette.testclient.TestClient object at 0x7a533ae3e120>

    def test_verify_api_key_valid_query_param(self, auth_client):
        """Test API key verification with valid query parameter"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info?api_key=test_key")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:119: AssertionError
___________ TestVerifyAPIKey.test_verify_api_key_logging_invalid_key ___________

self = <MagicMock name='logger.warning' id='134497888875248'>

    def assert_called(self):
        """assert that the mock was called at least once
        """
        if self.call_count == 0:
            msg = ("Expected '%s' to have been called." %
                   (self._mock_name or 'mock'))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to have been called.

/usr/lib/python3.12/unittest/mock.py:913: AssertionError

During handling of the above exception, another exception occurred:

self = <manual.test_auth.TestVerifyAPIKey object at 0x7a5405391580>

    def test_verify_api_key_logging_invalid_key(self):
        """Test that invalid API key attempts are logged"""
        # Standard library imports
        from unittest.mock import Mock
    
        from app.auth import verify_api_key
    
        with patch.dict(
            os.environ, {"API_KEYS": "valid_key", "REQUIRE_API_KEY": "true"}
        ), patch("app.auth.logger") as mock_logger:
            mock_request = Mock()
            mock_request.url.path = "/api/test"
            mock_request.client = Mock()
            mock_request.client.host = "192.168.1.1"
            mock_request.headers = {}
            mock_request.query_params = {}
    
            # This should raise an exception and log the invalid attempt
            try:
                verify_api_key(mock_request, None)
            except Exception:
                pass
    
            # Verify that warning was logged for invalid key attempt
>           mock_logger.warning.assert_called()
E           AssertionError: Expected 'warning' to have been called.

tests/manual/test_auth.py:145: AssertionError
__________ TestRequestLoggingMiddleware.test_dispatch_with_exception ___________

request = <Mock id='134497889493520'>

    async def failing_call_next(request):
>       raise Exception("Test error")
E       Exception: Test error

tests/manual/test_middleware.py:248: Exception

During handling of the above exception, another exception occurred:

self = <app.middleware.RequestLoggingMiddleware object at 0x7a533aed02c0>
request = <Mock id='134497889493520'>
call_next = <function TestRequestLoggingMiddleware.test_dispatch_with_exception.<locals>.failing_call_next at 0x7a533b06dd00>

    async def dispatch(
        self, request: Request, call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        start_time = time.time()
        request_id = request.headers.get(
            "X-Request-ID", f"req-{int(start_time * 1000)}"
        )
    
        logger.info(
            json.dumps(
                {
                    "event": "request_started",
                    "request_id": request_id,
                    "method": request.method,
                    "path": request.url.path,
                    "client_ip": self.get_client_id(request),
                    "timestamp": start_time,
                }
            )
        )
    
        try:
            response = await call_next(request)
    
            duration = time.time() - start_time
            logger.info(
                json.dumps(
                    {
                        "event": "request_completed",
                        "request_id": request_id,
                        "status_code": response.status_code,
                        "duration_ms": duration * 1000,
                        "timestamp": time.time(),
                    }
                )
            )
    
            response.headers["X-Request-ID"] = request_id
            return response
    
        except Exception as e:
            duration = time.time() - start_time
            logger.error(
                json.dumps(
                    {
                        "event": "request_failed",
                        "request_id": request_id,
                        "error": str(e),
                        "duration_ms": duration * 1000,
>                       "timestamp": time.time(),
                    }
                )
            )

../../test-repos/clinic/app/middleware.py:155: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1134: in __call__
    return self._mock_call(*args, **kwargs)
/usr/lib/python3.12/unittest/mock.py:1138: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='time' id='134497889485888'>, args = (), kwargs = {}
effect = <list_iterator object at 0x7a533aed18d0>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
>               result = next(effect)
E               StopIteration

/usr/lib/python3.12/unittest/mock.py:1195: StopIteration

The above exception was the direct cause of the following exception:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x7a54053b6660>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x7a533aed02c0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
            with pytest.raises(Exception, match="Test error"):
>               await middleware.dispatch(mock_request, failing_call_next)
E               RuntimeError: coroutine raised StopIteration

tests/manual/test_middleware.py:254: RuntimeError

During handling of the above exception, another exception occurred:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x7a54053b6660>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x7a533aed02c0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
>           with pytest.raises(Exception, match="Test error"):
E           AssertionError: Regex pattern did not match.
E            Regex: 'Test error'
E            Input: 'coroutine raised StopIteration'

tests/manual/test_middleware.py:253: AssertionError
______________ TestClinicalAssertionModel.test_load_model_success ______________

args = ()
kwargs = {'model': <app.model.ClinicalAssertionModel object at 0x7a533aedf8c0>}
coro = <coroutine object TestClinicalAssertionModel.test_load_model_success at 0x7a533b32dad0>
task = <Task finished name='Task-711' coro=<TestClinicalAssertionModel.test_load_model_success() done, defined at /usr/lib/py...ock.py:1402> exception=AttributeError("<module 'app.model'> does not have the attribute 'TextClassificationPipeline'")>

    @functools.wraps(func)
    def inner(*args, **kwargs):
        coro = func(*args, **kwargs)
        if not inspect.isawaitable(coro):
            pyfuncitem.warn(
                pytest.PytestWarning(
                    f"The test {pyfuncitem} is marked with '@pytest.mark.asyncio' "
                    "but it is not an async function. "
                    "Please remove asyncio marker. "
                    "If the test is not marked explicitly, "
                    "check for global markers applied via 'pytestmark'."
                )
            )
            return
        task = asyncio.ensure_future(coro, loop=_loop)
        try:
>           _loop.run_until_complete(task)

venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/asyncio/base_events.py:687: in run_until_complete
    return future.result()
/usr/lib/python3.12/unittest/mock.py:1404: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
/usr/lib/python3.12/unittest/mock.py:1369: in decoration_helper
    arg = exit_stack.enter_context(patching)
/usr/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
/usr/lib/python3.12/unittest/mock.py:1458: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <unittest.mock._patch object at 0x7a533b651850>

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
>           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: <module 'app.model'> does not have the attribute 'TextClassificationPipeline'

/usr/lib/python3.12/unittest/mock.py:1431: AttributeError
_________ TestClinicalAssertionModel.test_load_model_tokenizer_failure _________

args = ()
kwargs = {'model': <app.model.ClinicalAssertionModel object at 0x7a533b06b560>}
coro = <coroutine object TestClinicalAssertionModel.test_load_model_tokenizer_failure at 0x7a533b1f0040>
task = <Task finished name='Task-712' coro=<TestClinicalAssertionModel.test_load_model_tokenizer_failure() done, defined at /usr/lib/python3.12/unittest/mock.py:1402> exception=AttributeError("module 'app.model' has no attribute 'AutoTokenizer'")>

    @functools.wraps(func)
    def inner(*args, **kwargs):
        coro = func(*args, **kwargs)
        if not inspect.isawaitable(coro):
            pyfuncitem.warn(
                pytest.PytestWarning(
                    f"The test {pyfuncitem} is marked with '@pytest.mark.asyncio' "
                    "but it is not an async function. "
                    "Please remove asyncio marker. "
                    "If the test is not marked explicitly, "
                    "check for global markers applied via 'pytestmark'."
                )
            )
            return
        task = asyncio.ensure_future(coro, loop=_loop)
        try:
>           _loop.run_until_complete(task)

venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/asyncio/base_events.py:687: in run_until_complete
    return future.result()
/usr/lib/python3.12/unittest/mock.py:1404: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
/usr/lib/python3.12/unittest/mock.py:1369: in decoration_helper
    arg = exit_stack.enter_context(patching)
/usr/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
/usr/lib/python3.12/unittest/mock.py:1442: in __enter__
    self.target = self.getter()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'app.model.AutoTokenizer'

    def resolve_name(name):
        """
        Resolve a name to an object.
    
        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:
    
        W(.W)*
        W(.W)*:(W(.W)*)?
    
        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.
    
        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.
    
        The function will return an object (which might be a module), or raise one
        of the following exceptions:
    
        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)
    
        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
E           AttributeError: module 'app.model' has no attribute 'AutoTokenizer'

/usr/lib/python3.12/pkgutil.py:528: AttributeError
___________ TestClinicalAssertionModel.test_load_model_model_failure ___________

args = ()
kwargs = {'model': <app.model.ClinicalAssertionModel object at 0x7a533aed03b0>}
coro = <coroutine object TestClinicalAssertionModel.test_load_model_model_failure at 0x7a533b32f890>
task = <Task finished name='Task-713' coro=<TestClinicalAssertionModel.test_load_model_model_failure() done, defined at /usr/...est/mock.py:1402> exception=AttributeError("module 'app.model' has no attribute 'AutoModelForSequenceClassification'")>

    @functools.wraps(func)
    def inner(*args, **kwargs):
        coro = func(*args, **kwargs)
        if not inspect.isawaitable(coro):
            pyfuncitem.warn(
                pytest.PytestWarning(
                    f"The test {pyfuncitem} is marked with '@pytest.mark.asyncio' "
                    "but it is not an async function. "
                    "Please remove asyncio marker. "
                    "If the test is not marked explicitly, "
                    "check for global markers applied via 'pytestmark'."
                )
            )
            return
        task = asyncio.ensure_future(coro, loop=_loop)
        try:
>           _loop.run_until_complete(task)

venv/lib/python3.12/site-packages/pytest_asyncio/plugin.py:532: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/asyncio/base_events.py:687: in run_until_complete
    return future.result()
/usr/lib/python3.12/unittest/mock.py:1404: in patched
    with self.decoration_helper(patched,
/usr/lib/python3.12/contextlib.py:137: in __enter__
    return next(self.gen)
/usr/lib/python3.12/unittest/mock.py:1369: in decoration_helper
    arg = exit_stack.enter_context(patching)
/usr/lib/python3.12/contextlib.py:526: in enter_context
    result = _enter(cm)
/usr/lib/python3.12/unittest/mock.py:1442: in __enter__
    self.target = self.getter()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'app.model.AutoModelForSequenceClassification'

    def resolve_name(name):
        """
        Resolve a name to an object.
    
        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:
    
        W(.W)*
        W(.W)*:(W(.W)*)?
    
        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.
    
        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.
    
        The function will return an object (which might be a module), or raise one
        of the following exceptions:
    
        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)
    
        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
E           AttributeError: module 'app.model' has no attribute 'AutoModelForSequenceClassification'

/usr/lib/python3.12/pkgutil.py:528: AttributeError
_______________ TestClinicalAssertionModel.test_predict_success ________________

self = <manual.test_model.TestClinicalAssertionModel object at 0x7a533b670500>
mock_loop = <MagicMock name='get_event_loop' id='134497891171264'>
model = <app.model.ClinicalAssertionModel object at 0x7a533b0698e0>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_success(self, mock_loop, model):
        """Test successful prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock the pipeline result
        mock_result = {"label": "LABEL_0", "score": 0.95}
        model.pipeline.return_value = [mock_result]
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_result)
    
        result = await model.predict("test sentence")
    
>       assert result == {"label": "PRESENT", "score": 0.95}
E       AssertionError: assert {'label': 'LA...'score': 0.95} == {'label': 'PR...'score': 0.95}
E         Omitting 1 identical items, use -vv to show
E         Differing items:
E         {'label': 'LABEL_0'} != {'label': 'PRESENT'}
E         Full diff:
E         - {'label': 'PRESENT', 'score': 0.95}
E         ?            ^^ ^^^^
E         + {'label': 'LABEL_0', 'score': 0.95}
E         ?            ^^^ ^^^

tests/manual/test_model.py:145: AssertionError
____________ TestClinicalAssertionModel.test_predict_batch_success _____________

self = <manual.test_model.TestClinicalAssertionModel object at 0x7a533b671460>
mock_loop = <MagicMock name='get_event_loop' id='134497891319888'>
model = <app.model.ClinicalAssertionModel object at 0x7a533b0d0140>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_batch_success(self, mock_loop, model):
        """Test successful batch prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock batch results
        mock_results = [
            [{"label": "LABEL_0", "score": 0.95}],
            [{"label": "LABEL_1", "score": 0.87}],
        ]
        model.pipeline.return_value = mock_results
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_results)
    
        result = await model.predict_batch(["sentence 1", "sentence 2"])
    
        expected = [
            {"label": "PRESENT", "score": 0.95},
            {"label": "ABSENT", "score": 0.87},
        ]
>       assert result == expected
E       AssertionError: assert [[{'label': '...core': 0.87}]] == [{'label': 'P...score': 0.87}]
E         At index 0 diff: [{'label': 'LABEL_0', 'score': 0.95}] != {'label': 'PRESENT', 'score': 0.95}
E         Full diff:
E         - [{'label': 'PRESENT', 'score': 0.95}, {'label': 'ABSENT', 'score': 0.87}]
E         ?             ^^ ^^^^                                - ^^
E         + [[{'label': 'LABEL_0', 'score': 0.95}], [{'label': 'LABEL_1', 'score': 0.87}]]
E         ? +            ^^^ ^^^                 +  +           +   ^^^                  +

tests/manual/test_model.py:237: AssertionError
=============================== warnings summary ===============================
venv/lib/python3.12/site-packages/transformers/utils/generic.py:441
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

venv/lib/python3.12/site-packages/transformers/utils/generic.py:309
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
  /home/sigmoid/TECH_DEMO/new-tech-demo/tests/manual/test_auth.py:140: RuntimeWarning: coroutine 'verify_api_key' was never awaited
    verify_api_key(mock_request, None)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform linux, python 3.12.3-final-0 -----------
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           154     28     16      2    81%   73-100, 160, 333-336, 358, 432-437
/home/sigmoid/test-repos/clinic/app/middleware.py      92      0     14      0   100%
/home/sigmoid/test-repos/clinic/app/model.py           82     18     14      0    79%   36-64, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      3     10      3    92%   86, 90, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49      5     12      3    87%   26-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 487     69     84     10    84%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml

=========================== short test summary info ============================
FAILED tests/generated/test_integ_20251113_134428_01.py::test_model_info_endpoint_when_model_loaded_returns_info
FAILED tests/generated/test_integ_20251113_134428_01.py::test_general_exception_handler_increments_request_count_and_returns_500
FAILED tests/generated/test_unit_20251113_134428_01.py::test_predictionrequest_sentence_validation[long_string-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa-True]
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
FAILED tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_tokenizer_failure
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_model_failure
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_service_status_and_contains_expected_keys
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_root_initializing_and_then_healthy_state
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_metrics_endpoint_returns_prometheus_content_type
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_health_check_when_model_missing_returns_503
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_health_check_returns_healthy_when_model_loaded_and_system_metrics
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_model_info_responds_503_when_model_not_loaded
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_model_info_returns_info_when_loaded
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_predict_raises_503_when_model_not_loaded
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_predict_success_path_monkeypatched_pipeline_and_sanitizer
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_predict_handles_model_predict_exception_and_returns_500
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_predict_batch_rejects_over_max_size
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_predict_batch_success
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_system_metrics_reflects_model_loaded_and_system_values
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_model_info_requires_api_key_and_rejects_missing
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_sanitize_and_apply_pipeline_edge_cases
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentence_parametrized[normal sentence-True]
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentence_parametrized[-False]
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentence_parametrized[None-False]
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentences_parametrized[values0-2]
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentences_parametrized[values1-0]
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_validate_sentences_parametrized[None-0]
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_http_exception_handler_produces_structured_json
ERROR tests/generated/test_e2e_20251113_134428_01.py::test_general_exception_handler_catches_unexpected
======= 17 failed, 96 passed, 7 skipped, 3 warnings, 23 errors in 17.99s =======

ðŸ“Š Combined Coverage Analysis:
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           157     30     18      3    80%   73-100, 160, 333-336, 358, 432-437, 558-559
/home/sigmoid/test-repos/clinic/app/middleware.py      92      0     14      0   100%
/home/sigmoid/test-repos/clinic/app/model.py           82     18     14      0    79%   36-64, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      3     10      3    92%   86, 90, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49      5     12      3    87%   26-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 490     71     86     11    83%

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
FINAL RESULTS
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
âœ… Manual Test Coverage:   85.01%
âœ… Combined Coverage:      85.83%
ðŸ“ˆ Coverage Improvement:   0.82%

âš ï¸  Quality Gate: Coverage 85.83% < 90%
ðŸ’¡ Consider:
   1. Review uncovered code in htmlcov/index.html
   2. Add more manual tests for complex scenarios
   3. Re-run gap analysis for another iteration

âœ… Pipeline completed with coverage improvement
ðŸš€ Starting Enhanced Pipeline - Manual + Gap-Based AI Test Flow
===================================================================

ðŸŽ¯ Target Directory: /home/sigmoid/test-repos/clinic

ðŸ§¹ Cleaning previous coverage data...
âœ… Coverage data cleaned

ðŸ” Running detect_manual_tests.py on target repo...
ðŸ” Scanning repository for manual test directories in: /home/sigmoid/test-repos/clinic

ðŸ“Š Detection Result:
{
  "manual_tests_found": true,
  "manual_test_paths": [
    "/home/sigmoid/test-repos/clinic/tests"
  ],
  "test_files_count": 5,
  "test_dirs_detail": {
    "/home/sigmoid/test-repos/clinic/tests": [
      "/home/sigmoid/test-repos/clinic/tests/test_middleware.py",
      "/home/sigmoid/test-repos/clinic/tests/test_api.py",
      "/home/sigmoid/test-repos/clinic/tests/conftest.py",
      "/home/sigmoid/test-repos/clinic/tests/test_model.py",
      "/home/sigmoid/test-repos/clinic/tests/test_auth.py"
    ]
  }
}

ðŸ“ Manual Tests Found: True
ðŸ“‚ Test Paths: /home/sigmoid/test-repos/clinic/tests

âœ… Manual test cases detected. Running pytest with coverage analysis...

ðŸ“¦ Installing project dependencies...
Requirement already satisfied: fastapi==0.104.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (0.104.1)
Requirement already satisfied: uvicorn==0.24.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.24.0)
Requirement already satisfied: pydantic==2.5.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 4)) (2.5.0)
Requirement already satisfied: torch==2.4.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (2.4.0)
Requirement already satisfied: transformers==4.35.2 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (4.35.2)
Requirement already satisfied: tokenizers==0.15.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 9)) (0.15.0)
Requirement already satisfied: accelerate==0.24.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 10)) (0.24.1)
Requirement already satisfied: python-jose==3.3.0 in ./venv/lib/python3.12/site-packages (from python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (3.3.0)
Requirement already satisfied: python-multipart==0.0.6 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 14)) (0.0.6)
Requirement already satisfied: prometheus-client==0.19.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 17)) (0.19.0)
Requirement already satisfied: aiofiles==23.2.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 20)) (23.2.1)
Requirement already satisfied: httpx==0.25.2 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (0.25.2)
Requirement already satisfied: psutil==5.9.6 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 24)) (5.9.6)
Requirement already satisfied: pytest==7.4.3 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 27)) (7.4.3)
Requirement already satisfied: pytest-asyncio==0.21.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 28)) (0.21.1)
Requirement already satisfied: pytest-cov==4.1.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 29)) (4.1.0)
Requirement already satisfied: black==23.10.1 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (23.10.1)
Requirement already satisfied: isort==5.12.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 31)) (5.12.0)
Requirement already satisfied: flake8==6.1.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (6.1.0)
Requirement already satisfied: mypy==1.7.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 33)) (1.7.0)
Requirement already satisfied: bandit==1.7.5 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (1.7.5)
Requirement already satisfied: safety==2.3.4 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (2.3.4)
Requirement already satisfied: python-dotenv==1.0.0 in ./venv/lib/python3.12/site-packages (from -r /home/sigmoid/test-repos/clinic/requirements.txt (line 38)) (1.0.0)
Requirement already satisfied: anyio<4.0.0,>=3.7.1 in ./venv/lib/python3.12/site-packages (from fastapi==0.104.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (3.7.1)
Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./venv/lib/python3.12/site-packages (from fastapi==0.104.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (0.27.0)
Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from fastapi==0.104.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 2)) (4.15.0)
Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic==2.5.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 4)) (0.7.0)
Requirement already satisfied: pydantic-core==2.14.1 in ./venv/lib/python3.12/site-packages (from pydantic==2.5.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 4)) (2.14.1)
Requirement already satisfied: click>=7.0 in ./venv/lib/python3.12/site-packages (from uvicorn==0.24.0->uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (8.3.0)
Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.12/site-packages (from uvicorn==0.24.0->uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.16.0)
Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.20.0)
Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (1.14.0)
Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.5)
Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.1.6)
Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (2025.10.0)
Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (80.9.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (2.20.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.1.105)
Requirement already satisfied: triton==3.0.0 in ./venv/lib/python3.12/site-packages (from torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.0.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (0.36.0)
Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2.3.4)
Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2025.11.3)
Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2.32.5)
Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (0.6.2)
Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (4.67.1)
Requirement already satisfied: ecdsa!=0.15 in ./venv/lib/python3.12/site-packages (from python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (0.19.1)
Requirement already satisfied: rsa in ./venv/lib/python3.12/site-packages (from python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (4.9.1)
Requirement already satisfied: pyasn1 in ./venv/lib/python3.12/site-packages (from python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (0.6.1)
Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (2025.10.5)
Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (1.0.9)
Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (3.11)
Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from httpx==0.25.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 21)) (1.3.1)
Requirement already satisfied: iniconfig in ./venv/lib/python3.12/site-packages (from pytest==7.4.3->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 27)) (2.3.0)
Requirement already satisfied: pluggy<2.0,>=0.12 in ./venv/lib/python3.12/site-packages (from pytest==7.4.3->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 27)) (1.6.0)
Requirement already satisfied: coverage>=5.2.1 in ./venv/lib/python3.12/site-packages (from coverage[toml]>=5.2.1->pytest-cov==4.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 29)) (7.11.3)
Requirement already satisfied: mypy-extensions>=0.4.3 in ./venv/lib/python3.12/site-packages (from black==23.10.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (1.1.0)
Requirement already satisfied: pathspec>=0.9.0 in ./venv/lib/python3.12/site-packages (from black==23.10.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (0.12.1)
Requirement already satisfied: platformdirs>=2 in ./venv/lib/python3.12/site-packages (from black==23.10.1->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 30)) (4.5.0)
Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from flake8==6.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (0.7.0)
Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in ./venv/lib/python3.12/site-packages (from flake8==6.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (2.11.1)
Requirement already satisfied: pyflakes<3.2.0,>=3.1.0 in ./venv/lib/python3.12/site-packages (from flake8==6.1.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 32)) (3.1.0)
Requirement already satisfied: GitPython>=1.0.1 in ./venv/lib/python3.12/site-packages (from bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (3.1.45)
Requirement already satisfied: stevedore>=1.20.0 in ./venv/lib/python3.12/site-packages (from bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (5.5.0)
Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (14.2.0)
Requirement already satisfied: dparse>=0.6.2 in ./venv/lib/python3.12/site-packages (from safety==2.3.4->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (0.6.4)
Requirement already satisfied: ruamel.yaml>=0.17.21 in ./venv/lib/python3.12/site-packages (from safety==2.3.4->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (0.18.16)
Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (12.9.86)
Requirement already satisfied: cryptography>=3.4.0 in ./venv/lib/python3.12/site-packages (from python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (46.0.3)
Requirement already satisfied: httptools>=0.5.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.7.1)
Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (0.22.1)
Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (1.1.1)
Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]==0.24.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 3)) (15.0.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (1.2.0)
Requirement already satisfied: cffi>=2.0.0 in ./venv/lib/python3.12/site-packages (from cryptography>=3.4.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (2.0.0)
Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (2.23)
Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.12/site-packages (from ecdsa!=0.15->python-jose==3.3.0->python-jose[cryptography]==3.3.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 13)) (1.10.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.12/site-packages (from GitPython>=1.0.1->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (4.0.12)
Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.1->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (5.0.2)
Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in ./venv/lib/python3.12/site-packages (from ruamel.yaml>=0.17.21->safety==2.3.4->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 35)) (0.2.14)
Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (3.0.3)
Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (3.4.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers==4.35.2->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 8)) (2.5.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (4.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (2.19.2)
Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->bandit==1.7.5->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 34)) (0.1.2)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->torch==2.4.0->-r /home/sigmoid/test-repos/clinic/requirements.txt (line 7)) (1.3.0)

ðŸ“‚ Copying manual tests to local folder: ./tests/manual

âœ… Manual test files copied to ./tests/manual
ðŸ“„ Files:
./tests/manual/test_middleware.py
./tests/manual/test_api.py
./tests/manual/test_model.py
./tests/manual/test_auth.py

ðŸ§ª Running manual tests from local directory with coverage analysis: ./tests/manual

============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.3, pluggy-1.6.0 -- /home/sigmoid/TECH_DEMO/new-tech-demo/venv/bin/python3
cachedir: .pytest_cache
metadata: {'Python': '3.12.3', 'Platform': 'Linux-6.14.0-33-generic-x86_64-with-glibc2.39', 'Packages': {'pytest': '7.4.3', 'pluggy': '1.6.0'}, 'Plugins': {'html': '4.1.1', 'django': '4.11.1', 'asyncio': '0.21.1', 'mock': '3.15.1', 'anyio': '3.7.1', 'metadata': '3.1.1', 'cov': '4.1.0'}}
rootdir: /home/sigmoid/TECH_DEMO/new-tech-demo
configfile: pytest.ini
plugins: html-4.1.1, django-4.11.1, asyncio-0.21.1, mock-3.15.1, anyio-3.7.1, metadata-3.1.1, cov-4.1.0
asyncio: mode=Mode.AUTO
collecting ... collected 88 items

tests/manual/test_api.py::TestHealthEndpoint::test_health_check_success PASSED [  1%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_success PASSED [  2%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_empty_sentence PASSED [  3%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[The patient denies chest pain.] PASSED [  4%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[He has a history of hypertension.] PASSED [  5%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[No signs of pneumonia were observed.] PASSED [  6%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_success PASSED [  7%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_empty_list PASSED [  9%]
tests/manual/test_api.py::TestRootEndpoint::test_root_endpoint PASSED    [ 10%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_conditional PASSED [ 11%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_uncertainty PASSED [ 12%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_no_rule PASSED [ 13%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_multiple_sentences PASSED [ 14%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_positive PASSED [ 15%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_negative PASSED [ 17%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_positive PASSED [ 18%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_negative PASSED [ 19%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_predict_with_hybrid_pipeline PASSED [ 20%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_batch_predict_with_hybrid_pipeline PASSED [ 21%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_no_api_keys PASSED  [ 22%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_api_keys PASSED [ 23%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_whitespace_api_keys PASSED [ 25%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_no_keys_required PASSED [ 26%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_valid PASSED  [ 27%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_invalid PASSED [ 28%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint FAILED [ 29%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_root_endpoint PASSED [ 30%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_metrics_endpoint PASSED [ 31%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required FAILED [ 32%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request FAILED [ 34%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid FAILED [ 35%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer FAILED [ 36%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header FAILED [ 37%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param FAILED [ 38%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key FAILED [ 39%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_multiple_keys PASSED [ 40%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_empty_key_string PASSED [ 42%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_whitespace_only PASSED [ 43%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_special_characters PASSED [ 44%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_case_sensitive PASSED [ 45%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init PASSED [ 46%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init_with_csp PASSED [ 47%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_no_csp_http PASSED [ 48%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_with_csp_https PASSED [ 50%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_init PASSED [ 51%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 52%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_client PASSED [ 53%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_unknown PASSED [ 54%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_health_endpoint PASSED [ 55%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_under_limit PASSED [ 56%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_over_limit PASSED [ 57%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_init PASSED [ 59%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 60%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_client PASSED [ 61%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_unknown PASSED [ 62%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_success PASSED [ 63%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception FAILED [ 64%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_init PASSED [ 65%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_success PASSED [ 67%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_error PASSED [ 68%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_exception PASSED [ 69%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_init PASSED [ 70%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_not_loaded PASSED [ 71%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_after_init PASSED [ 72%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_no_model PASSED [ 73%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_available PASSED [ 75%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_not_available PASSED [ 76%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success FAILED [ 77%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_tokenizer_failure PASSED [ 78%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_model_failure PASSED [ 79%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_not_loaded PASSED [ 80%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_not_loaded PASSED [ 81%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success FAILED [ 82%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_pipeline_error PASSED [ 84%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_success PASSED [ 85%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_list_result PASSED [ 86%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_nested_list_result PASSED [ 87%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unexpected_result_type PASSED [ 88%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success FAILED [ 89%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_success PASSED [ 90%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_single_result PASSED [ 92%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info PASSED [ 93%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info_cuda_available PASSED [ 94%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_empty_result PASSED [ 95%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_none_result PASSED [ 96%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_empty_results PASSED [ 97%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_mixed_result_types PASSED [ 98%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unknown_label PASSED [100%]

=================================== FAILURES ===================================
_____________ TestVerifyAPIKey.test_verify_api_key_health_endpoint _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af24080>
auth_client = <starlette.testclient.TestClient object at 0x74344079a9f0>

    def test_verify_api_key_health_endpoint(self, auth_client):
        """Test that health endpoint doesn't require API key"""
        response = auth_client.get("/health")
>       assert response.status_code == 200
E       assert 503 == 200
E        +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:55: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.main:main.py:229 Health check failed:
____________ TestVerifyAPIKey.test_verify_api_key_no_keys_required _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af24bc0>
auth_client = <starlette.testclient.TestClient object at 0x74344079aae0>

    def test_verify_api_key_no_keys_required(self, auth_client):
        """Test endpoints when no API keys are required"""
        with patch.dict(os.environ, {"API_KEYS": "", "REQUIRE_API_KEY": "false"}):
            response = auth_client.get("/model/info")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:71: AssertionError
__________ TestVerifyAPIKey.test_verify_api_key_missing_from_request ___________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af24f50>
auth_client = <starlette.testclient.TestClient object at 0x743440799a90>

    def test_verify_api_key_missing_from_request(self, auth_client):
        """Test API key verification when key is missing"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info")
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:79: AssertionError
_________________ TestVerifyAPIKey.test_verify_api_key_invalid _________________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af25280>
auth_client = <starlette.testclient.TestClient object at 0x743440507380>

    def test_verify_api_key_invalid(self, auth_client):
        """Test API key verification with invalid key"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer invalid_key"}
            )
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:91: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_bearer _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af25610>
auth_client = <starlette.testclient.TestClient object at 0x743440505c70>

    def test_verify_api_key_valid_bearer(self, auth_client):
        """Test API key verification with valid Bearer token"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer test_key"}
            )
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:103: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_header _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af25970>
auth_client = <starlette.testclient.TestClient object at 0x743440bfb350>

    def test_verify_api_key_valid_header(self, auth_client):
        """Test API key verification with valid X-API-Key header"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info", headers={"X-API-Key": "test_key"})
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:111: AssertionError
____________ TestVerifyAPIKey.test_verify_api_key_valid_query_param ____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af1ffe0>
auth_client = <starlette.testclient.TestClient object at 0x743440511490>

    def test_verify_api_key_valid_query_param(self, auth_client):
        """Test API key verification with valid query parameter"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info?api_key=test_key")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:119: AssertionError
___________ TestVerifyAPIKey.test_verify_api_key_logging_invalid_key ___________

self = <MagicMock name='logger.warning' id='127767770862064'>

    def assert_called(self):
        """assert that the mock was called at least once
        """
        if self.call_count == 0:
            msg = ("Expected '%s' to have been called." %
                   (self._mock_name or 'mock'))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to have been called.

/usr/lib/python3.12/unittest/mock.py:913: AssertionError

During handling of the above exception, another exception occurred:

self = <manual.test_auth.TestVerifyAPIKey object at 0x74350af1f590>

    def test_verify_api_key_logging_invalid_key(self):
        """Test that invalid API key attempts are logged"""
        # Standard library imports
        from unittest.mock import Mock
    
        from app.auth import verify_api_key
    
        with patch.dict(
            os.environ, {"API_KEYS": "valid_key", "REQUIRE_API_KEY": "true"}
        ), patch("app.auth.logger") as mock_logger:
            mock_request = Mock()
            mock_request.url.path = "/api/test"
            mock_request.client = Mock()
            mock_request.client.host = "192.168.1.1"
            mock_request.headers = {}
            mock_request.query_params = {}
    
            # This should raise an exception and log the invalid attempt
            try:
                verify_api_key(mock_request, None)
            except Exception:
                pass
    
            # Verify that warning was logged for invalid key attempt
>           mock_logger.warning.assert_called()
E           AssertionError: Expected 'warning' to have been called.

tests/manual/test_auth.py:145: AssertionError
__________ TestRequestLoggingMiddleware.test_dispatch_with_exception ___________

request = <Mock id='127767766192144'>

    async def failing_call_next(request):
>       raise Exception("Test error")
E       Exception: Test error

tests/manual/test_middleware.py:248: Exception

During handling of the above exception, another exception occurred:

self = <app.middleware.RequestLoggingMiddleware object at 0x7434405149e0>
request = <Mock id='127767766192144'>
call_next = <function TestRequestLoggingMiddleware.test_dispatch_with_exception.<locals>.failing_call_next at 0x74343fa58ea0>

    async def dispatch(
        self, request: Request, call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        start_time = time.time()
        request_id = request.headers.get(
            "X-Request-ID", f"req-{int(start_time * 1000)}"
        )
    
        logger.info(
            json.dumps(
                {
                    "event": "request_started",
                    "request_id": request_id,
                    "method": request.method,
                    "path": request.url.path,
                    "client_ip": self.get_client_id(request),
                    "timestamp": start_time,
                }
            )
        )
    
        try:
            response = await call_next(request)
    
            duration = time.time() - start_time
            logger.info(
                json.dumps(
                    {
                        "event": "request_completed",
                        "request_id": request_id,
                        "status_code": response.status_code,
                        "duration_ms": duration * 1000,
                        "timestamp": time.time(),
                    }
                )
            )
    
            response.headers["X-Request-ID"] = request_id
            return response
    
        except Exception as e:
            duration = time.time() - start_time
            logger.error(
                json.dumps(
                    {
                        "event": "request_failed",
                        "request_id": request_id,
                        "error": str(e),
                        "duration_ms": duration * 1000,
>                       "timestamp": time.time(),
                    }
                )
            )

../../test-repos/clinic/app/middleware.py:155: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1134: in __call__
    return self._mock_call(*args, **kwargs)
/usr/lib/python3.12/unittest/mock.py:1138: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='time' id='127767766190464'>, args = (), kwargs = {}
effect = <list_iterator object at 0x74344051df30>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
>               result = next(effect)
E               StopIteration

/usr/lib/python3.12/unittest/mock.py:1195: StopIteration

The above exception was the direct cause of the following exception:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x74350ac47ec0>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x7434405149e0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
            with pytest.raises(Exception, match="Test error"):
>               await middleware.dispatch(mock_request, failing_call_next)
E               RuntimeError: coroutine raised StopIteration

tests/manual/test_middleware.py:254: RuntimeError

During handling of the above exception, another exception occurred:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x74350ac47ec0>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x7434405149e0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
>           with pytest.raises(Exception, match="Test error"):
E           AssertionError: Regex pattern did not match.
E            Regex: 'Test error'
E            Input: 'coroutine raised StopIteration'

tests/manual/test_middleware.py:253: AssertionError
______________ TestClinicalAssertionModel.test_load_model_success ______________

self = <manual.test_model.TestClinicalAssertionModel object at 0x743440bd4770>
mock_cuda = <MagicMock name='is_available' id='127767766231296'>
mock_pipeline = <MagicMock name='TextClassificationPipeline' id='127767766224912'>
mock_model_class = <MagicMock name='from_pretrained' id='127771212455408'>
mock_tokenizer = <MagicMock name='from_pretrained' id='127767766178928'>
model = <app.model.ClinicalAssertionModel object at 0x74344051e870>

    @pytest.mark.asyncio
    @patch("app.model.AutoTokenizer.from_pretrained")
    @patch("app.model.AutoModelForSequenceClassification.from_pretrained")
    @patch("app.model.TextClassificationPipeline")
    @patch("torch.cuda.is_available")
    async def test_load_model_success(
        self, mock_cuda, mock_pipeline, mock_model_class, mock_tokenizer, model
    ):
        """Test successful model loading"""
        mock_cuda.return_value = False
    
        # Mock the model and tokenizer
        mock_model_instance = Mock()
        mock_model_class.return_value = mock_model_instance
        mock_tokenizer_instance = Mock()
        mock_tokenizer.return_value = mock_tokenizer_instance
        mock_pipeline_instance = Mock()
        mock_pipeline.return_value = mock_pipeline_instance
    
        await model.load_model()
    
        assert model.tokenizer == mock_tokenizer_instance
>       assert model.model == mock_model_instance
E       AssertionError: assert <Mock name='from_pretrained().to().to()' id='127767766187232'> == <Mock name='from_pretrained()' id='127767766183440'>
E        +  where <Mock name='from_pretrained().to().to()' id='127767766187232'> = <app.model.ClinicalAssertionModel object at 0x74344051e870>.model

tests/manual/test_model.py:82: AssertionError
_______________ TestClinicalAssertionModel.test_predict_success ________________

self = <manual.test_model.TestClinicalAssertionModel object at 0x743440bd7830>
mock_loop = <MagicMock name='get_event_loop' id='127767756754336'>
model = <app.model.ClinicalAssertionModel object at 0x74343fc16e40>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_success(self, mock_loop, model):
        """Test successful prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock the pipeline result
        mock_result = {"label": "LABEL_0", "score": 0.95}
        model.pipeline.return_value = [mock_result]
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_result)
    
        result = await model.predict("test sentence")
    
>       assert result == {"label": "PRESENT", "score": 0.95}
E       AssertionError: assert {'label': 'LA...'score': 0.95} == {'label': 'PR...'score': 0.95}
E         Omitting 1 identical items, use -vv to show
E         Differing items:
E         {'label': 'LABEL_0'} != {'label': 'PRESENT'}
E         Full diff:
E         - {'label': 'PRESENT', 'score': 0.95}
E         ?            ^^ ^^^^
E         + {'label': 'LABEL_0', 'score': 0.95}
E         ?            ^^^ ^^^

tests/manual/test_model.py:145: AssertionError
____________ TestClinicalAssertionModel.test_predict_batch_success _____________

self = <manual.test_model.TestClinicalAssertionModel object at 0x743440be4560>
mock_loop = <MagicMock name='get_event_loop' id='127767766134912'>
model = <app.model.ClinicalAssertionModel object at 0x743440506de0>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_batch_success(self, mock_loop, model):
        """Test successful batch prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock batch results
        mock_results = [
            [{"label": "LABEL_0", "score": 0.95}],
            [{"label": "LABEL_1", "score": 0.87}],
        ]
        model.pipeline.return_value = mock_results
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_results)
    
        result = await model.predict_batch(["sentence 1", "sentence 2"])
    
        expected = [
            {"label": "PRESENT", "score": 0.95},
            {"label": "ABSENT", "score": 0.87},
        ]
>       assert result == expected
E       AssertionError: assert [[{'label': '...core': 0.87}]] == [{'label': 'P...score': 0.87}]
E         At index 0 diff: [{'label': 'LABEL_0', 'score': 0.95}] != {'label': 'PRESENT', 'score': 0.95}
E         Full diff:
E         - [{'label': 'PRESENT', 'score': 0.95}, {'label': 'ABSENT', 'score': 0.87}]
E         ?             ^^ ^^^^                                - ^^
E         + [[{'label': 'LABEL_0', 'score': 0.95}], [{'label': 'LABEL_1', 'score': 0.87}]]
E         ? +            ^^^ ^^^                 +  +           +   ^^^                  +

tests/manual/test_model.py:237: AssertionError
=============================== warnings summary ===============================
venv/lib/python3.12/site-packages/transformers/utils/generic.py:441
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

venv/lib/python3.12/site-packages/transformers/utils/generic.py:309
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
  /home/sigmoid/TECH_DEMO/new-tech-demo/tests/manual/test_auth.py:140: RuntimeWarning: coroutine 'verify_api_key' was never awaited
    verify_api_key(mock_request, None)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform linux, python 3.12.3-final-0 -----------
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           154     40     16      5    72%   73-100, 160, 176-178, 257, 277-278, 333-336, 358, 365, 432-437, 454-457, 536-542
/home/sigmoid/test-repos/clinic/app/middleware.py      92      1     14      0    99%   159
/home/sigmoid/test-repos/clinic/app/model.py           82      3     14      1    96%   45->48, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      4     10      4    90%   37, 86, 90, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49     10     12      3    79%   15-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 487     73     84     15    82%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml

=========================== short test summary info ============================
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
FAILED tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success
================== 12 failed, 76 passed, 3 warnings in 19.88s ==================
ðŸ“Š Coverage report generated
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           157     42     18      6    71%   73-100, 160, 176-178, 257, 277-278, 333-336, 358, 365, 432-437, 454-457, 536-542, 558-559
/home/sigmoid/test-repos/clinic/app/middleware.py      92      1     14      0    99%   159
/home/sigmoid/test-repos/clinic/app/model.py           82      3     14      1    96%   45->48, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      4     10      4    90%   37, 86, 90, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49     10     12      3    79%   15-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 490     75     86     16    82%

âœ… Pytest completed for manual tests.

âœ… Manual Test Coverage: 85.01%

ðŸ“Š Coverage Summary:
  app/__init__.py: 100.0%
  app/auth.py: 61.5%
  app/main.py: 74.0%
  app/middleware.py: 98.9%
  app/model.py: 96.3%
  app/schemas.py: 94.3%
  app/utils.py: 79.6%

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
COVERAGE ANALYSIS PHASE
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
hello
ðŸ” Analyzing coverage gaps...
ðŸ” Analyzing coverage gaps...
================================================================================
COVERAGE GAP ANALYSIS REPORT
================================================================================

Overall Coverage: 85.01%
Total Statements: 487
Covered Statements: 414
Missing Statements: 73
AI Generation Needed: YES

FILES WITH COVERAGE GAPS:
--------------------------------------------------------------------------------

ðŸ“ app/auth.py
   Coverage: 61.54%
   Missing Lines: 15
   Uncovered: 42, 47, 49-50, 52-53, 55-56, 58-59, 63-66, 70

ðŸ“ app/main.py
   Coverage: 74.03%
   Missing Lines: 40
   Uncovered: 73-75, 77, 79-80, 83-84, 86-87, 89-91, 93, 96-97, 99-100, 160, 176, 178, 257, 277-278, 333-336, 358, 365, 432-433, 436-437, 454-455, 457, 536, 540, 542

ðŸ“ app/middleware.py
   Coverage: 98.91%
   Missing Lines: 1
   Uncovered: 159

ðŸ“ app/model.py
   Coverage: 96.34%
   Missing Lines: 3
   Uncovered: 117-119

ðŸ“ app/schemas.py
   Coverage: 94.29%
   Missing Lines: 4
   Uncovered: 37, 86, 90, 94

ðŸ“ app/utils.py
   Coverage: 79.59%
   Missing Lines: 10
   Uncovered: 15-18, 20, 26-28, 39, 55



================================================================================
âœ… Coverage gap analysis saved to: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json

âš ï¸  Coverage is below 90% (85.01%)
ðŸ¤– AI test generation recommended for uncovered code

âš ï¸  Coverage is below 90%
ðŸ¤– Initiating Gap-Based AI Test Generation...

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
GAP-BASED AI TEST GENERATION (Using Full AI Workflow)
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80

Warning: postprocess import failed: cannot import name 'extract_python_only' from 'src.gen.postprocess' (/home/sigmoid/TECH_DEMO/new-tech-demo/src/gen/postprocess.py); using fallbacks
ðŸŽ¯ Gap-focused mode enabled via --coverage-mode argument
Found 14 Python files in target directory
Project structure: Universal compatibility enabled
UNIVERSAL analysis for ANY PROJECT STRUCTURE in: /home/sigmoid/test-repos/clinic
Analyzing 8 Python files in project...
[framework_manager] Detection error in django: 'str' object has no attribute 'get'
[framework_manager] Framework(s) detected: ['fastapi', 'universal'] -> selected: fastapi
UNIVERSAL ANALYSIS COMPLETE:
   Files analyzed: 8
   Functions: 42 (top-level)
   Nested Functions: 23
   Classes: 28
   Methods: 23
   Routes: 8
   FastAPI Routes: 10
   Properties: 0
   Async functions: 39
   Imports tracked: 104
   Django models: 0
   Serializers: 0
   Views/ViewSets: 0
   Forms: 0
   Admin: 0
   Project packages: 2
   Detected Framework: fastapi
Starting UNIVERSAL test generation with gap-focused coverage mode...
UNIVERSAL test generation for ANY PROJECT STRUCTURE...

ðŸŽ¯ GAP-FOCUSED MODE: Analyzing coverage gaps...

================================================================================
GAP-FOCUSED MODE ACTIVE
================================================================================
Filtering analysis to target only uncovered code...
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6

ðŸŽ¯ Filtering analysis to focus on coverage gaps...

ðŸ“Š Gap-Focused Analysis Results:
   Original Functions: 42 â†’ Uncovered: 0
   Original Classes: 28 â†’ Uncovered: 0
   Original Methods: 23 â†’ Uncovered: 0
   Original Routes: 8 â†’ Uncovered: 8
   Total Reduction: 100.0% (focusing only on gaps)

âœ… Gap-focused analysis prepared
   Targeting 73 uncovered statements
================================================================================

âœ… Gap analysis complete: Targeting 0 uncovered functions, 0 uncovered classes, 0 uncovered methods
âœ… Successfully wrote: tests/generated/conftest.py
Created universal conftest: tests/generated/conftest.py
TESTGEN_FORCE environment variable: 'true'
ðŸš€ Force generation enabled - will regenerate all tests
Found 8 source files for force generation:
  - app/__init__.py
  - app/auth.py
  - app/main.py
  - app/middleware.py
  - app/model.py
  - app/schemas.py
  - app/utils.py
  - scripts/health-check.py
ðŸ§¹ Force mode: Cleaning up all existing generated tests
Cleaned up 0 existing test files
UNIVERSAL TARGET INCLUSION:
   Functions: 4 (including 4 nested)
   Classes: 0
   Methods: 0
   Routes: 18
   Project Packages: 2
Using universal targeting - all targets included
Analyzing required packages...
   auth: Local module (skipped pip install)
   middleware: Local module (skipped pip install)
   model: Local module (skipped pip install)
   schemas: Local module (skipped pip install)
   utils: Local module (skipped pip install)
Inferred 9 external packages: fastapi, httpx, prometheus_client, psutil, pydantic, starlette, torch, transformers, uvicorn
Installing 9 external packages...
Installing packages: fastapi, httpx, prometheus_client, psutil, pydantic, starlette, torch, transformers, uvicorn
   fastapi
   httpx
   prometheus_client
   psutil
   pydantic
   starlette
   torch
   transformers
   uvicorn
Successfully installed 9 packages.
UNIVERSAL COVERAGE TARGETS:
   Functions: 4
   Classes: 0
   Methods: 0
   Routes: 18
   Total Targets: 22
   Expected Coverage: Maximum
   Project Structure: Universal compatibility
Generating 1 UNIT test files for universal compatibility...
Generating unit test 1/1 for 4 targets
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
   ðŸ“Š Added 1691 chars of gap-focused context to prompt
âš ï¸ Syntax error in generated code for tests/generated/test_unit_20251113_135905_01.py: expected an indented block after 'if' statement on line 24 (<unknown>, line 27)
âœ… Fixed syntax errors in tests/generated/test_unit_20251113_135905_01.py
âœ… Successfully wrote: tests/generated/test_unit_20251113_135905_01.py
  test_unit_20251113_135905_01.py - 4 targets
Generating 1 INTEG test files for universal compatibility...
Generating integ test 1/1 for 18 targets
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
   ðŸ“Š Added 1691 chars of gap-focused context to prompt
âš ï¸ Syntax error in generated code for tests/generated/test_integ_20251113_135905_01.py: expected an indented block after 'if' statement on line 28 (<unknown>, line 31)
âœ… Fixed syntax errors in tests/generated/test_integ_20251113_135905_01.py
âœ… Successfully wrote: tests/generated/test_integ_20251113_135905_01.py
  test_integ_20251113_135905_01.py - 18 targets
Generating 1 E2E test files for universal compatibility...
Generating e2e test 1/1 for 18 targets
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
âœ… Loaded coverage gaps analysis from: /home/sigmoid/TECH_DEMO/new-tech-demo/coverage_gaps.json
   Current Coverage: 85.01%
   Uncovered Functions: 0
   Uncovered Classes: 0
   Files with Gaps: 6
   ðŸ“Š Added 1691 chars of gap-focused context to prompt
âš ï¸ Syntax error in generated code for tests/generated/test_e2e_20251113_135905_01.py: expected an indented block after 'if' statement on line 22 (<unknown>, line 25)
âœ… Fixed syntax errors in tests/generated/test_e2e_20251113_135905_01.py
âœ… Successfully wrote: tests/generated/test_e2e_20251113_135905_01.py
  test_e2e_20251113_135905_01.py - 18 targets
Updated test mapping for 8 source files
Generation completed: 3 test files for 8 source files
ðŸ“Š Updated test manifest: tests/generated/_manifest.json
UNIVERSAL GENERATION COMPLETE: 3 test files
Expected Coverage: Maximum with REAL IMPORTS
Universal Compatibility: ENABLED
Targets Covered: 22
Project Structure: 2 packages detected
UNIVERSAL TEST GENERATION SUCCESSFUL!
UNIVERSAL Results:
   Generated: 3 test files
   Coverage Mode: gap-focused
   Universal Compatibility: ENABLED
   Targets Covered: 101
   Project Structure: Universal handling enabled
Run UNIVERSAL Tests:
   Basic: python -m pytest ./tests/generated -v
   Coverage: python -m pytest ./tests/generated --cov=. --cov-report=html
   Universal: PYTHONPATH=/home/sigmoid/test-repos/clinic python -m pytest ./tests/generated
=== AI Test Generation Completed ===
ðŸ§© Total AI-generated test files: 3
./tests/generated/test_integ_20251113_135905_01.py
./tests/generated/test_unit_20251113_135905_01.py
./tests/generated/test_e2e_20251113_135905_01.py

âœ… Gap-based AI test generation completed successfully

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
RUNNING COMBINED TESTS (Manual + AI Generated)
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80

ðŸ“¦ Installing project dependencies...
ðŸ§ª Running combined test suite...
============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-7.4.3, pluggy-1.6.0 -- /home/sigmoid/TECH_DEMO/new-tech-demo/venv/bin/python3
cachedir: .pytest_cache
django: version: 5.2.8
metadata: {'Python': '3.12.3', 'Platform': 'Linux-6.14.0-33-generic-x86_64-with-glibc2.39', 'Packages': {'pytest': '7.4.3', 'pluggy': '1.6.0'}, 'Plugins': {'html': '4.1.1', 'django': '4.11.1', 'asyncio': '0.21.1', 'mock': '3.15.1', 'anyio': '3.7.1', 'metadata': '3.1.1', 'cov': '4.1.0'}}
rootdir: /home/sigmoid/TECH_DEMO/new-tech-demo
configfile: pytest.ini
plugins: html-4.1.1, django-4.11.1, asyncio-0.21.1, mock-3.15.1, anyio-3.7.1, metadata-3.1.1, cov-4.1.0
asyncio: mode=Mode.AUTO
collecting ... collected 142 items

tests/generated/test_e2e_20251113_135905_01.py::test_service_status_endpoint PASSED [  0%]
tests/generated/test_e2e_20251113_135905_01.py::test_health_check_model_not_loaded FAILED [  1%]
tests/generated/test_e2e_20251113_135905_01.py::test_health_check_model_loaded PASSED [  2%]
tests/generated/test_e2e_20251113_135905_01.py::test_model_info_when_not_loaded PASSED [  2%]
tests/generated/test_e2e_20251113_135905_01.py::test_model_info_when_loaded FAILED [  3%]
tests/generated/test_e2e_20251113_135905_01.py::test_metrics_endpoint PASSED [  4%]
tests/generated/test_e2e_20251113_135905_01.py::test_predict_assertion_success_and_failure PASSED [  4%]
tests/generated/test_e2e_20251113_135905_01.py::test_predict_batch_success_and_batch_size_limit PASSED [  5%]
tests/generated/test_e2e_20251113_135905_01.py::test_system_metrics_endpoint PASSED [  6%]
tests/generated/test_e2e_20251113_135905_01.py::test_root_status_reflects_model_state PASSED [  7%]
tests/generated/test_e2e_20251113_135905_01.py::test_validate_sentence_variants[Normal sentence-True] SKIPPED [  7%]
tests/generated/test_e2e_20251113_135905_01.py::test_validate_sentence_variants[-False] SKIPPED [  8%]
tests/generated/test_e2e_20251113_135905_01.py::test_validate_sentence_variants[None-False] SKIPPED [  9%]
tests/generated/test_e2e_20251113_135905_01.py::test_validate_sentences_edgecases SKIPPED [  9%]
tests/generated/test_e2e_20251113_135905_01.py::test_clinical_model_predict_batch_direct FAILED [ 10%]
tests/generated/test_e2e_20251113_135905_01.py::test_general_exception_handler_metrics_increment PASSED [ 11%]
tests/generated/test_e2e_20251113_135905_01.py::test_middleware_dispatch_error_path FAILED [ 11%]
tests/generated/test_integ_20251113_135905_01.py::test_service_status_endpoint_basic PASSED [ 12%]
tests/generated/test_integ_20251113_135905_01.py::test_health_check_when_model_not_loaded_triggers_http_exception_handler FAILED [ 13%]
tests/generated/test_integ_20251113_135905_01.py::test_health_check_when_model_loaded_returns_health_response PASSED [ 14%]
tests/generated/test_integ_20251113_135905_01.py::test_metrics_endpoint_returns_prometheus_content_type PASSED [ 14%]
tests/generated/test_integ_20251113_135905_01.py::test_model_info_endpoint_when_model_missing_returns_503 PASSED [ 15%]
tests/generated/test_integ_20251113_135905_01.py::test_model_info_endpoint_success_returns_model_info FAILED [ 16%]
tests/generated/test_integ_20251113_135905_01.py::test_predict_assertion_returns_503_when_model_unavailable PASSED [ 16%]
tests/generated/test_integ_20251113_135905_01.py::test_predict_assertion_success_and_failure_paths PASSED [ 17%]
tests/generated/test_integ_20251113_135905_01.py::test_predict_batch_invalid_and_success_cases PASSED [ 18%]
tests/generated/test_integ_20251113_135905_01.py::test_system_metrics_endpoint_and_root_status_reflect_model_state PASSED [ 19%]
tests/generated/test_integ_20251113_135905_01.py::test_http_exception_handler_direct_invocation_increments_metrics FAILED [ 19%]
tests/generated/test_integ_20251113_135905_01.py::test_general_exception_handler_direct_invocation_produces_500_json FAILED [ 20%]
tests/generated/test_integ_20251113_135905_01.py::test_validate_sentence_and_sentences_variants[Normal sentence.-True] SKIPPED [ 21%]
tests/generated/test_integ_20251113_135905_01.py::test_validate_sentence_and_sentences_variants[-False] SKIPPED [ 21%]
tests/generated/test_integ_20251113_135905_01.py::test_validate_sentence_and_sentences_variants[None-False] SKIPPED [ 22%]
tests/generated/test_integ_20251113_135905_01.py::test_validate_sentence_and_sentences_variants[   -False] SKIPPED [ 23%]
tests/generated/test_integ_20251113_135905_01.py::test_validate_sentence_and_sentences_variants[Symptom: cough-True] SKIPPED [ 23%]
tests/generated/test_integ_20251113_135905_01.py::test_utils_sanitize_and_apply_hybrid_pipeline_edge_cases FAILED [ 24%]
tests/generated/test_integ_20251113_135905_01.py::test_verify_api_key_missing_and_invalid PASSED [ 25%]
tests/generated/test_integ_20251113_135905_01.py::test_request_triggers_middleware_dispatch_and_logging PASSED [ 26%]
tests/generated/test_integ_20251113_135905_01.py::test_real_model_predict_batch_if_available SKIPPED [ 26%]
tests/generated/test_integ_20251113_135905_01.py::test_module_level_attributes_exist PASSED [ 27%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentence_via_model[Normal clinical sentence.-True] PASSED [ 28%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentence_via_model[-False] PASSED [ 28%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentence_via_model[None-False] PASSED [ 29%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentence_via_model[   Leading and trailing   -True] PASSED [ 30%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentence_via_model[Short-True] PASSED [ 30%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentences_via_batch_model[sentences0-True] PASSED [ 31%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentences_via_batch_model[sentences1-False] PASSED [ 32%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentences_via_batch_model[sentences2-False] PASSED [ 33%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentences_via_batch_model[sentences3-False] PASSED [ 33%]
tests/generated/test_unit_20251113_135905_01.py::test_validate_sentences_via_batch_model[sentences4-True] PASSED [ 34%]
tests/generated/test_unit_20251113_135905_01.py::test_predict_batch_behavior[input_sentences0-False] FAILED [ 35%]
tests/generated/test_unit_20251113_135905_01.py::test_predict_batch_behavior[input_sentences1-False] FAILED [ 35%]
tests/generated/test_unit_20251113_135905_01.py::test_predict_batch_behavior[input_sentences2-True] PASSED [ 36%]
tests/generated/test_unit_20251113_135905_01.py::test_middleware_dispatch_error_path PASSED [ 37%]
tests/generated/test_unit_20251113_135905_01.py::test_schema_str_and_repr_exist PASSED [ 38%]
tests/manual/test_api.py::TestHealthEndpoint::test_health_check_success PASSED [ 38%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_success PASSED [ 39%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_empty_sentence PASSED [ 40%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[The patient denies chest pain.] PASSED [ 40%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[He has a history of hypertension.] PASSED [ 41%]
tests/manual/test_api.py::TestPredictionEndpoint::test_predict_various_sentences[No signs of pneumonia were observed.] PASSED [ 42%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_success PASSED [ 42%]
tests/manual/test_api.py::TestBatchPredictionEndpoint::test_batch_predict_empty_list PASSED [ 43%]
tests/manual/test_api.py::TestRootEndpoint::test_root_endpoint PASSED    [ 44%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_conditional PASSED [ 45%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_uncertainty PASSED [ 45%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_no_rule PASSED [ 46%]
tests/manual/test_api.py::TestHybridPipeline::test_apply_hybrid_pipeline_multiple_sentences PASSED [ 47%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_positive PASSED [ 47%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_conditional_phrases_negative PASSED [ 48%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_positive PASSED [ 49%]
tests/manual/test_api.py::TestUtilityFunctions::test_detect_uncertainty_phrases_negative PASSED [ 50%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_predict_with_hybrid_pipeline PASSED [ 50%]
tests/manual/test_api.py::TestEnhancedPredictionEndpoints::test_batch_predict_with_hybrid_pipeline PASSED [ 51%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_no_api_keys PASSED  [ 52%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_api_keys PASSED [ 52%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_init_with_whitespace_api_keys PASSED [ 53%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_no_keys_required PASSED [ 54%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_valid PASSED  [ 54%]
tests/manual/test_auth.py::TestAPIKeyAuth::test_verify_key_invalid PASSED [ 55%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint FAILED [ 56%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_root_endpoint PASSED [ 57%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_metrics_endpoint PASSED [ 57%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required FAILED [ 58%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request FAILED [ 59%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid FAILED [ 59%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer FAILED [ 60%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header FAILED [ 61%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param FAILED [ 61%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key FAILED [ 62%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_multiple_keys PASSED [ 63%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_empty_key_string PASSED [ 64%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_whitespace_only PASSED [ 64%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_special_characters PASSED [ 65%]
tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_case_sensitive PASSED [ 66%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init PASSED [ 66%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_init_with_csp PASSED [ 67%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_no_csp_http PASSED [ 68%]
tests/manual/test_middleware.py::TestSecurityHeadersMiddleware::test_dispatch_with_csp_https PASSED [ 69%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_init PASSED [ 69%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 70%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_with_client PASSED [ 71%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_get_client_id_unknown PASSED [ 71%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_health_endpoint PASSED [ 72%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_under_limit PASSED [ 73%]
tests/manual/test_middleware.py::TestRateLimitMiddleware::test_dispatch_over_limit PASSED [ 73%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_init PASSED [ 74%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_x_forwarded_for PASSED [ 75%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_with_client PASSED [ 76%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_get_client_id_unknown PASSED [ 76%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_success PASSED [ 77%]
tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception FAILED [ 78%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_init PASSED [ 78%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_success PASSED [ 79%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_error PASSED [ 80%]
tests/manual/test_middleware.py::TestMetricsMiddleware::test_dispatch_exception PASSED [ 80%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_init PASSED [ 81%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_not_loaded PASSED [ 82%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_after_init PASSED [ 83%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_is_loaded_no_model PASSED [ 83%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_available PASSED [ 84%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_device_cuda_not_available PASSED [ 85%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success FAILED [ 85%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_tokenizer_failure PASSED [ 86%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_model_failure PASSED [ 87%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_not_loaded PASSED [ 88%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_not_loaded PASSED [ 88%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success FAILED [ 89%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_pipeline_error PASSED [ 90%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_success PASSED [ 90%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_list_result PASSED [ 91%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_nested_list_result PASSED [ 92%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unexpected_result_type PASSED [ 92%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success FAILED [ 93%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_success PASSED [ 94%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_single_result PASSED [ 95%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info PASSED [ 95%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_get_model_info_cuda_available PASSED [ 96%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_empty_result PASSED [ 97%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_none_result PASSED [ 97%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_empty_results PASSED [ 98%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_sync_mixed_result_types PASSED [ 99%]
tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_sync_unknown_label PASSED [100%]

=================================== FAILURES ===================================
______________________ test_health_check_model_not_loaded ______________________

client_factory = <function client_factory.<locals>._create at 0x778a0e3bd800>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e390080>

    def test_health_check_model_not_loaded(client_factory, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        # Ensure model is None to trigger the not-loaded branch
        set_global_model(monkeypatch, None)
        client = client_factory()
        resp = client.get("/health")
        assert resp.status_code == 503
        body = resp.json()
        # The HTTPException handler will have been invoked by FastAPI machinery
>       assert "Model not loaded" in body.get("error", body.get("detail", "")) or "Model not loaded" in str(body)
E       assert ('Model not loaded' in 'Health check failed: ' or 'Model not loaded' in "{'error': 'Health check failed: ', 'status_code': 503, 'timestamp': 1763042685.4038928, 'path': '/health'}")
E        +  where 'Health check failed: ' = <built-in method get of dict object at 0x778a0e2573c0>('error', '')
E        +    where <built-in method get of dict object at 0x778a0e2573c0> = {'error': 'Health check failed: ', 'path': '/health', 'status_code': 503, 'timestamp': 1763042685.4038928}.get
E        +    and   '' = <built-in method get of dict object at 0x778a0e2573c0>('detail', '')
E        +      where <built-in method get of dict object at 0x778a0e2573c0> = {'error': 'Health check failed: ', 'path': '/health', 'status_code': 503, 'timestamp': 1763042685.4038928}.get
E        +  and   "{'error': 'Health check failed: ', 'status_code': 503, 'timestamp': 1763042685.4038928, 'path': '/health'}" = str({'error': 'Health check failed: ', 'path': '/health', 'status_code': 503, 'timestamp': 1763042685.4038928})

tests/generated/test_e2e_20251113_135905_01.py:148: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.main:main.py:229 Health check failed:
_________________________ test_model_info_when_loaded __________________________

client_factory = <function client_factory.<locals>._create at 0x778a0e35d260>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e285dc0>

    def test_model_info_when_loaded(client_factory, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        stub = StubClinicalAssertionModel(loaded=True)
        set_global_model(monkeypatch, stub)
        client = client_factory()
        resp = client.get("/model/info")
>       assert resp.status_code == 200
E       assert 500 == 200
E        +  where 500 = <Response [500 Internal Server Error]>.status_code

tests/generated/test_e2e_20251113_135905_01.py:182: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.middleware:middleware.py:148 {"event": "request_failed", "request_id": "req-1763042685631", "error": "4 validation errors for ModelInfoResponse\nmodel_name\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\ndevice\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\nloaded\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\ncuda_available\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing", "duration_ms": 1.1720657348632812, "timestamp": 1763042685.6331809}
ERROR    app.main:main.py:540 Unhandled exception: 4 validation errors for ModelInfoResponse
model_name
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
device
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
loaded
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
cuda_available
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 98, in receive
    return self.receive_nowait()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 93, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 78, in call_next
    message = await recv_stream.receive()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 118, in receive
    raise EndOfStream
anyio.EndOfStream

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 176, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 128, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/gzip.py", line 24, in __call__
    await responder(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/gzip.py", line 44, in __call__
    await self.app(scope, receive, self.send_with_gzip)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/trustedhost.py", line 34, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 30, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 79, in __call__
    raise exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 68, in __call__
    await self.app(scope, receive, sender)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 20, in __call__
    raise e
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 17, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 718, in __call__
    await route.handle(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 276, in handle
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 66, in app
    response = await func(request)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/routing.py", line 274, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/routing.py", line 191, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/main.py", line 257, in model_info
    return ModelInfoResponse(**model.get_model_info())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/pydantic/main.py", line 164, in __init__
    __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)
pydantic_core._pydantic_core.ValidationError: 4 validation errors for ModelInfoResponse
model_name
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
device
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
loaded
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
cuda_available
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
___________________ test_clinical_model_predict_batch_direct ___________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e5c0fe0>

    def test_clinical_model_predict_batch_direct(monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        """Directly exercise ClinicalAssertionModel.predict_batch in isolation by patching heavy internals if needed."""
        model_module = pytest.importorskip("app.model")
        ClinicalAssertionModel = getattr(model_module, "ClinicalAssertionModel", None)
        if ClinicalAssertionModel is None:
            pytest.skip("ClinicalAssertionModel not defined in app.model")
    
        # Instantiate the real class if lightweight or fallback to stub
        try:
            inst = ClinicalAssertionModel()
        except Exception:
            # Fallback to stub if real class fails to instantiate
            inst = StubClinicalAssertionModel(loaded=True)
    
        # Patch instance methods that may rely on external libs to deterministic behavior
        if not hasattr(inst, "predict_batch") or not asyncio.iscoroutinefunction(getattr(inst, "predict_batch")):
            # Provide an async predict_batch on the instance
            async def _pb(sentences):
                return [{"label": "ABSENT", "model_label": "absent", "score": 0.5, "sentence": s} for s in sentences]
            inst.predict_batch = _pb
    
        # Call with empty list to hit edge behavior and with non-empty list
        loop = asyncio.new_event_loop()
        try:
            asyncio.set_event_loop(loop)
>           res_empty = loop.run_until_complete(inst.predict_batch([]))

tests/generated/test_e2e_20251113_135905_01.py:358: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/asyncio/base_events.py:687: in run_until_complete
    return future.result()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <app.model.ClinicalAssertionModel object at 0x778a0e31fc80>
sentences = []

    async def predict_batch(self, sentences: List[str]) -> List[Dict[str, Any]]:
        """Predict assertion status for multiple sentences"""
        if not self.is_loaded():
>           raise RuntimeError("Model is not loaded")
E           RuntimeError: Model is not loaded

../../test-repos/clinic/app/model.py:109: RuntimeError
_____________________ test_middleware_dispatch_error_path ______________________

client_factory = <function client_factory.<locals>._create at 0x778a0e35e3e0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e7be480>

    def test_middleware_dispatch_error_path(client_factory, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        """
        Attempt to trigger middleware dispatch path that handles unexpected exceptions.
        We simulate by temporarily inserting a middleware that raises inside dispatch and ensuring
        the app's exception handlers still respond appropriately.
        """
        from starlette.middleware.base import BaseHTTPMiddleware
    
        class BrokenMiddleware(BaseHTTPMiddleware):
            async def dispatch(self, request, call_next):
                # Force an exception to flow through middleware
                raise RuntimeError("middleware induced failure")
    
        # Insert BrokenMiddleware at a low level
        app = app_module.app
>       app.add_middleware(BrokenMiddleware)

tests/generated/test_e2e_20251113_135905_01.py:407: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <fastapi.applications.FastAPI object at 0x778a0e6034a0>
middleware_class = <class 'test_e2e_20251113_135905_01.test_middleware_dispatch_error_path.<locals>.BrokenMiddleware'>
options = {}

    def add_middleware(self, middleware_class: type, **options: typing.Any) -> None:
        if self.middleware_stack is not None:  # pragma: no cover
>           raise RuntimeError("Cannot add middleware after an application has started")
E           RuntimeError: Cannot add middleware after an application has started

venv/lib/python3.12/site-packages/starlette/applications.py:139: RuntimeError
___ test_health_check_when_model_not_loaded_triggers_http_exception_handler ____

    def test_health_check_when_model_not_loaded_triggers_http_exception_handler():
        """UNIVERSAL test for maximum coverage."""
        """
        When model is None, /health should respond with 503 via HTTPException;
        verifies http_exception_handler path for raising 503 (covered uncovered lines).
        """
        client = TestClient(app_main.app)
        # Ensure model is None
        app_main.model = None
        resp = client.get("/health")
        assert resp.status_code == 503
        body = resp.json()
        # http_exception_handler returns 'error' in content
>       assert "error" in body and "Model not loaded" in body["error"]
E       AssertionError: assert ('error' in {'error': 'Health check failed: ', 'path': '/health', 'status_code': 503, 'timestamp': 1763042686.1226456} and 'Model not loaded' in 'Health check failed: ')

tests/generated/test_integ_20251113_135905_01.py:177: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.main:main.py:229 Health check failed:
_____________ test_model_info_endpoint_success_returns_model_info ______________

self = MemoryObjectReceiveStream(_state=MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=0, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)

    async def receive(self) -> T_co:
        await checkpoint()
        try:
>           return self.receive_nowait()

venv/lib/python3.12/site-packages/anyio/streams/memory.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = MemoryObjectReceiveStream(_state=MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=0, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)

    def receive_nowait(self) -> T_co:
        """
        Receive the next item if it can be done without waiting.
    
        :return: the received item
        :raises ~anyio.ClosedResourceError: if this send stream has been closed
        :raises ~anyio.EndOfStream: if the buffer is empty and this stream has been
            closed from the sending end
        :raises ~anyio.WouldBlock: if there are no items in the buffer and no tasks
            waiting to send
    
        """
        if self._closed:
            raise ClosedResourceError
    
        if self._state.waiting_senders:
            # Get the item from the next sender
            send_event, item = self._state.waiting_senders.popitem(last=False)
            self._state.buffer.append(item)
            send_event.set()
    
        if self._state.buffer:
            return self._state.buffer.popleft()
        elif not self._state.open_send_channels:
            raise EndOfStream
    
>       raise WouldBlock
E       anyio.WouldBlock

venv/lib/python3.12/site-packages/anyio/streams/memory.py:93: WouldBlock

During handling of the above exception, another exception occurred:

request = <starlette.requests.Request object at 0x778a0e286150>

    async def call_next(request: Request) -> Response:
        app_exc: typing.Optional[Exception] = None
        send_stream, recv_stream = anyio.create_memory_object_stream()
    
        async def receive_or_disconnect() -> Message:
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            async with anyio.create_task_group() as task_group:
    
                async def wrap(func: typing.Callable[[], typing.Awaitable[T]]) -> T:
                    result = await func()
                    task_group.cancel_scope.cancel()
                    return result
    
                task_group.start_soon(wrap, response_sent.wait)
                message = await wrap(request.receive)
    
            if response_sent.is_set():
                return {"type": "http.disconnect"}
    
            return message
    
        async def close_recv_stream_on_response_sent() -> None:
            await response_sent.wait()
            recv_stream.close()
    
        async def send_no_error(message: Message) -> None:
            try:
                await send_stream.send(message)
            except anyio.BrokenResourceError:
                # recv_stream has been closed, i.e. response_sent has been set.
                return
    
        async def coro() -> None:
            nonlocal app_exc
    
            async with send_stream:
                try:
                    await self.app(scope, receive_or_disconnect, send_no_error)
                except Exception as exc:
                    app_exc = exc
    
        task_group.start_soon(close_recv_stream_on_response_sent)
        task_group.start_soon(coro)
    
        try:
>           message = await recv_stream.receive()

venv/lib/python3.12/site-packages/starlette/middleware/base.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = MemoryObjectReceiveStream(_state=MemoryObjectStreamState(max_buffer_size=0, buffer=deque([]), open_send_channels=0, open_receive_channels=1, waiting_receivers=OrderedDict(), waiting_senders=OrderedDict()), _closed=False)

    async def receive(self) -> T_co:
        await checkpoint()
        try:
            return self.receive_nowait()
        except WouldBlock:
            # Add ourselves in the queue
            receive_event = Event()
            container: list[T_co] = []
            self._state.waiting_receivers[receive_event] = container
    
            try:
                await receive_event.wait()
            except get_cancelled_exc_class():
                # Ignore the immediate cancellation if we already received an item, so as not to
                # lose it
                if not container:
                    raise
            finally:
                self._state.waiting_receivers.pop(receive_event, None)
    
            if container:
                return container[0]
            else:
>               raise EndOfStream
E               anyio.EndOfStream

venv/lib/python3.12/site-packages/anyio/streams/memory.py:118: EndOfStream

During handling of the above exception, another exception occurred:

    def test_model_info_endpoint_success_returns_model_info():
        """UNIVERSAL test for maximum coverage."""
        """
        With a loaded stub model, /model/info returns 200 and model information mapping.
        """
        app_main.model = StubModel(loaded=True)
        client = TestClient(app_main.app)
>       resp = client.get("/model/info")

tests/generated/test_integ_20251113_135905_01.py:238: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
venv/lib/python3.12/site-packages/starlette/testclient.py:499: in get
    return super().get(
venv/lib/python3.12/site-packages/httpx/_client.py:1041: in get
    return self.request(
venv/lib/python3.12/site-packages/starlette/testclient.py:465: in request
    return super().request(
venv/lib/python3.12/site-packages/httpx/_client.py:814: in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
venv/lib/python3.12/site-packages/httpx/_client.py:901: in send
    response = self._send_handling_auth(
venv/lib/python3.12/site-packages/httpx/_client.py:929: in _send_handling_auth
    response = self._send_handling_redirects(
venv/lib/python3.12/site-packages/httpx/_client.py:966: in _send_handling_redirects
    response = self._send_single_request(request)
venv/lib/python3.12/site-packages/httpx/_client.py:1002: in _send_single_request
    response = transport.handle_request(request)
venv/lib/python3.12/site-packages/starlette/testclient.py:342: in handle_request
    raise exc
venv/lib/python3.12/site-packages/starlette/testclient.py:339: in handle_request
    portal.call(self.app, scope, receive, send)
venv/lib/python3.12/site-packages/anyio/from_thread.py:277: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
/usr/lib/python3.12/concurrent/futures/_base.py:456: in result
    return self.__get_result()
/usr/lib/python3.12/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
venv/lib/python3.12/site-packages/anyio/from_thread.py:217: in _call_func
    retval = await retval
venv/lib/python3.12/site-packages/fastapi/applications.py:1106: in __call__
    await super().__call__(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/applications.py:122: in __call__
    await self.middleware_stack(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/errors.py:184: in __call__
    raise exc
venv/lib/python3.12/site-packages/starlette/middleware/errors.py:162: in __call__
    await self.app(scope, receive, _send)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:108: in __call__
    response = await self.dispatch_func(request, call_next)
../../test-repos/clinic/app/middleware.py:176: in dispatch
    response = await call_next(request)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:84: in call_next
    raise app_exc
venv/lib/python3.12/site-packages/starlette/middleware/base.py:70: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:108: in __call__
    response = await self.dispatch_func(request, call_next)
../../test-repos/clinic/app/middleware.py:128: in dispatch
    response = await call_next(request)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:84: in call_next
    raise app_exc
venv/lib/python3.12/site-packages/starlette/middleware/base.py:70: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
venv/lib/python3.12/site-packages/starlette/middleware/gzip.py:24: in __call__
    await responder(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/gzip.py:44: in __call__
    await self.app(scope, receive, self.send_with_gzip)
venv/lib/python3.12/site-packages/starlette/middleware/cors.py:83: in __call__
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/trustedhost.py:34: in __call__
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:108: in __call__
    response = await self.dispatch_func(request, call_next)
../../test-repos/clinic/app/middleware.py:30: in dispatch
    response = await call_next(request)
venv/lib/python3.12/site-packages/starlette/middleware/base.py:84: in call_next
    raise app_exc
venv/lib/python3.12/site-packages/starlette/middleware/base.py:70: in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py:79: in __call__
    raise exc
venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py:68: in __call__
    await self.app(scope, receive, sender)
venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py:20: in __call__
    raise e
venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py:17: in __call__
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/routing.py:718: in __call__
    await route.handle(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/routing.py:276: in handle
    await self.app(scope, receive, send)
venv/lib/python3.12/site-packages/starlette/routing.py:66: in app
    response = await func(request)
venv/lib/python3.12/site-packages/fastapi/routing.py:274: in app
    raw_response = await run_endpoint_function(
venv/lib/python3.12/site-packages/fastapi/routing.py:191: in run_endpoint_function
    return await dependant.call(**values)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @app.get(
        "/model/info",
        response_model=ModelInfoResponse,
        tags=["Model"],
        dependencies=[Depends(verify_api_key)] if os.getenv("REQUIRE_API_KEY") else [],
    )
    async def model_info() -> ModelInfoResponse:
        """Get detailed model information"""
        if not model or not model.is_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="Model not loaded"
            )
    
>       return ModelInfoResponse(**model.get_model_info())
E       pydantic_core._pydantic_core.ValidationError: 4 validation errors for ModelInfoResponse
E       model_name
E         Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing
E       device
E         Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing
E       loaded
E         Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing
E       cuda_available
E         Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
E           For further information visit https://errors.pydantic.dev/2.5/v/missing

../../test-repos/clinic/app/main.py:257: ValidationError
------------------------------ Captured log call -------------------------------
ERROR    app.middleware:middleware.py:148 {"event": "request_failed", "request_id": "req-1763042687280", "error": "4 validation errors for ModelInfoResponse\nmodel_name\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\ndevice\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\nloaded\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing\ncuda_available\n  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.5/v/missing", "duration_ms": 2.102375030517578, "timestamp": 1763042687.282158}
ERROR    app.main:main.py:540 Unhandled exception: 4 validation errors for ModelInfoResponse
model_name
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
device
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
loaded
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
cuda_available
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 98, in receive
    return self.receive_nowait()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 93, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 78, in call_next
    message = await recv_stream.receive()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/anyio/streams/memory.py", line 118, in receive
    raise EndOfStream
anyio.EndOfStream

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 162, in __call__
    await self.app(scope, receive, _send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 176, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 128, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/gzip.py", line 24, in __call__
    await responder(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/gzip.py", line 44, in __call__
    await self.app(scope, receive, self.send_with_gzip)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/cors.py", line 83, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/trustedhost.py", line 34, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 108, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/middleware.py", line 30, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 84, in call_next
    raise app_exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 70, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 79, in __call__
    raise exc
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 68, in __call__
    await self.app(scope, receive, sender)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 20, in __call__
    raise e
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 17, in __call__
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 718, in __call__
    await route.handle(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 276, in handle
    await self.app(scope, receive, send)
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/starlette/routing.py", line 66, in app
    response = await func(request)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/routing.py", line 274, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/fastapi/routing.py", line 191, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/test-repos/clinic/app/main.py", line 257, in model_info
    return ModelInfoResponse(**model.get_model_info())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/pydantic/main.py", line 164, in __init__
    __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)
pydantic_core._pydantic_core.ValidationError: 4 validation errors for ModelInfoResponse
model_name
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
device
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
loaded
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
cuda_available
  Field required [type=missing, input_value={'name': 'stub-model', 'v..., 'ABSENT', 'POSSIBLE']}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.5/v/missing
_______ test_http_exception_handler_direct_invocation_increments_metrics _______

    def test_http_exception_handler_direct_invocation_increments_metrics():
        """UNIVERSAL test for maximum coverage."""
        """
        Invoke http_exception_handler directly to ensure it builds the expected JSON structure.
        Tests the uncovered lines in http_exception_handler.
        """
        req = make_request({"headers": [(b"accept", b"application/json")]})
        exc = HTTPException(status_code=418, detail="I'm a teapot")
        # Call handler directly (it is an async function)
        handler = app_main.http_exception_handler
>       result = asyncio.get_event_loop().run_until_complete(handler(req, exc))

tests/generated/test_integ_20251113_135905_01.py:356: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/asyncio/base_events.py:662: in run_until_complete
    self._check_closed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=True debug=False>

    def _check_closed(self):
        if self._closed:
>           raise RuntimeError('Event loop is closed')
E           RuntimeError: Event loop is closed

/usr/lib/python3.12/asyncio/base_events.py:541: RuntimeError
______ test_general_exception_handler_direct_invocation_produces_500_json ______

    def test_general_exception_handler_direct_invocation_produces_500_json():
        """UNIVERSAL test for maximum coverage."""
        """
        Invoke general_exception_handler directly with a plain Exception to assert JSON error shape.
        """
        req = make_request({"headers": [(b"accept", b"application/json")]})
        exc = Exception("unexpected")
        handler = app_main.general_exception_handler
>       result = asyncio.get_event_loop().run_until_complete(handler(req, exc))

tests/generated/test_integ_20251113_135905_01.py:374: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/asyncio/base_events.py:662: in run_until_complete
    self._check_closed()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <_UnixSelectorEventLoop running=False closed=True debug=False>

    def _check_closed(self):
        if self._closed:
>           raise RuntimeError('Event loop is closed')
E           RuntimeError: Event loop is closed

/usr/lib/python3.12/asyncio/base_events.py:541: RuntimeError
___________ test_utils_sanitize_and_apply_hybrid_pipeline_edge_cases ___________

    def test_utils_sanitize_and_apply_hybrid_pipeline_edge_cases():
        """UNIVERSAL test for maximum coverage."""
        """
        Call sanitize_clinical_text and apply_hybrid_pipeline with edge cases to exercise
        different code paths in utils (uncovered lines).
        """
        if not app_utils:
            pytest.skip("app.utils not available")
        sanitize = getattr(app_utils, "sanitize_clinical_text", None)
        apply_hybrid = getattr(app_utils, "apply_hybrid_pipeline", None)
        get_sys = getattr(app_utils, "get_system_metrics", None)
    
        if sanitize:
            # Edge cases: empty, None, containing HTML, long strings
            assert sanitize("Normal text") != ""
>           assert sanitize("<script>alert(1)</script>") != "<script>alert(1)</script>"
E           AssertionError: assert '<script>alert(1)</script>' != '<script>alert(1)</script>'
E            +  where '<script>alert(1)</script>' = <function sanitize_clinical_text at 0x778ad865ef20>('<script>alert(1)</script>')

tests/generated/test_integ_20251113_135905_01.py:443: AssertionError
_____________ test_predict_batch_behavior[input_sentences0-False] ______________

input_sentences = [], expect_exception = False
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e5a2de0>

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "input_sentences,expect_exception",
        [
            ([], False),  # empty batch - some implementations return empty list
            (["Test sentence."], False),  # normal path
            (["", None], True),  # invalid content may cause error
        ],
    )
    async def test_predict_batch_behavior(input_sentences, expect_exception, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        """
        Exercise ClinicalAssertionModel.predict_batch (or module-level predict_batch).
        The goal is to hit various internal branches: empty input, normal input, invalid input.
        If heavy model loading would occur, we attempt to instantiate and then monkeypatch
        model internals minimally to avoid external dependencies.
        """
        model_mod = safe_import("app.model")
    
        # Determine if there's a top-level function predict_batch
        target_func = None
        if hasattr(model_mod, "predict_batch") and inspect.iscoroutinefunction(
            getattr(model_mod, "predict_batch")
        ):
            target_func = getattr(model_mod, "predict_batch")
    
        ClinicalAssertionModel = getattr(model_mod, "ClinicalAssertionModel", None)
    
        # If there's a class method, prefer calling that to hit uncovered lines which often live there.
        if ClinicalAssertionModel is None and target_func is None:
            pytest.skip("No predict_batch function or ClinicalAssertionModel available in app.model")
    
        # Prepare a minimal model instance if needed
        if ClinicalAssertionModel:
            # Create an instance without running heavy __init__ if possible
            try:
                model_instance = ClinicalAssertionModel()
            except Exception:
                # If constructor is heavy, create a simple object and attach the original method
                model_instance = SimpleNamespace()
                # If a method exists on the class, bind it
                if hasattr(ClinicalAssertionModel, "predict_batch"):
                    original = ClinicalAssertionModel.predict_batch
    
                    async def bound_predict_batch(self, sentences):
                        return await original(model_instance, sentences)
    
                    model_instance.predict_batch = bound_predict_batch
    
            # Monkeypatch heavy internals if they exist to avoid external resources
            # For example, if model_instance has tokenizer or pipeline attributes, stub them
            for attr in ("tokenizer", "model", "pipeline", "nlp"):
                if not hasattr(model_instance, attr):
                    setattr(model_instance, attr, Mock())
    
            # If predict_batch exists and is coroutine, call it directly
            if hasattr(model_instance, "predict_batch") and inspect.iscoroutinefunction(
                model_instance.predict_batch
            ):
                try:
>                   result = await model_instance.predict_batch(input_sentences)

tests/generated/test_unit_20251113_135905_01.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <app.model.ClinicalAssertionModel object at 0x778a0e563a70>
sentences = []

    async def predict_batch(self, sentences: List[str]) -> List[Dict[str, Any]]:
        """Predict assertion status for multiple sentences"""
        if not self.is_loaded():
>           raise RuntimeError("Model is not loaded")
E           RuntimeError: Model is not loaded

../../test-repos/clinic/app/model.py:109: RuntimeError

During handling of the above exception, another exception occurred:

input_sentences = [], expect_exception = False
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e5a2de0>

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "input_sentences,expect_exception",
        [
            ([], False),  # empty batch - some implementations return empty list
            (["Test sentence."], False),  # normal path
            (["", None], True),  # invalid content may cause error
        ],
    )
    async def test_predict_batch_behavior(input_sentences, expect_exception, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        """
        Exercise ClinicalAssertionModel.predict_batch (or module-level predict_batch).
        The goal is to hit various internal branches: empty input, normal input, invalid input.
        If heavy model loading would occur, we attempt to instantiate and then monkeypatch
        model internals minimally to avoid external dependencies.
        """
        model_mod = safe_import("app.model")
    
        # Determine if there's a top-level function predict_batch
        target_func = None
        if hasattr(model_mod, "predict_batch") and inspect.iscoroutinefunction(
            getattr(model_mod, "predict_batch")
        ):
            target_func = getattr(model_mod, "predict_batch")
    
        ClinicalAssertionModel = getattr(model_mod, "ClinicalAssertionModel", None)
    
        # If there's a class method, prefer calling that to hit uncovered lines which often live there.
        if ClinicalAssertionModel is None and target_func is None:
            pytest.skip("No predict_batch function or ClinicalAssertionModel available in app.model")
    
        # Prepare a minimal model instance if needed
        if ClinicalAssertionModel:
            # Create an instance without running heavy __init__ if possible
            try:
                model_instance = ClinicalAssertionModel()
            except Exception:
                # If constructor is heavy, create a simple object and attach the original method
                model_instance = SimpleNamespace()
                # If a method exists on the class, bind it
                if hasattr(ClinicalAssertionModel, "predict_batch"):
                    original = ClinicalAssertionModel.predict_batch
    
                    async def bound_predict_batch(self, sentences):
                        return await original(model_instance, sentences)
    
                    model_instance.predict_batch = bound_predict_batch
    
            # Monkeypatch heavy internals if they exist to avoid external resources
            # For example, if model_instance has tokenizer or pipeline attributes, stub them
            for attr in ("tokenizer", "model", "pipeline", "nlp"):
                if not hasattr(model_instance, attr):
                    setattr(model_instance, attr, Mock())
    
            # If predict_batch exists and is coroutine, call it directly
            if hasattr(model_instance, "predict_batch") and inspect.iscoroutinefunction(
                model_instance.predict_batch
            ):
                try:
                    result = await model_instance.predict_batch(input_sentences)
                    if expect_exception:
                        # We expected an exception but got a result
                        assert False, f"Expected exception but got result: {result}"
                    else:
                        # Validate result format if possible - expect a list of dict-like results
                        assert isinstance(result, list)
                except Exception:
                    if not expect_exception:
>                       pytest.fail("predict_batch raised unexpectedly")
E                       Failed: predict_batch raised unexpectedly

tests/generated/test_unit_20251113_135905_01.py:209: Failed
_____________ test_predict_batch_behavior[input_sentences1-False] ______________

input_sentences = ['Test sentence.'], expect_exception = False
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e5a0560>

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "input_sentences,expect_exception",
        [
            ([], False),  # empty batch - some implementations return empty list
            (["Test sentence."], False),  # normal path
            (["", None], True),  # invalid content may cause error
        ],
    )
    async def test_predict_batch_behavior(input_sentences, expect_exception, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        """
        Exercise ClinicalAssertionModel.predict_batch (or module-level predict_batch).
        The goal is to hit various internal branches: empty input, normal input, invalid input.
        If heavy model loading would occur, we attempt to instantiate and then monkeypatch
        model internals minimally to avoid external dependencies.
        """
        model_mod = safe_import("app.model")
    
        # Determine if there's a top-level function predict_batch
        target_func = None
        if hasattr(model_mod, "predict_batch") and inspect.iscoroutinefunction(
            getattr(model_mod, "predict_batch")
        ):
            target_func = getattr(model_mod, "predict_batch")
    
        ClinicalAssertionModel = getattr(model_mod, "ClinicalAssertionModel", None)
    
        # If there's a class method, prefer calling that to hit uncovered lines which often live there.
        if ClinicalAssertionModel is None and target_func is None:
            pytest.skip("No predict_batch function or ClinicalAssertionModel available in app.model")
    
        # Prepare a minimal model instance if needed
        if ClinicalAssertionModel:
            # Create an instance without running heavy __init__ if possible
            try:
                model_instance = ClinicalAssertionModel()
            except Exception:
                # If constructor is heavy, create a simple object and attach the original method
                model_instance = SimpleNamespace()
                # If a method exists on the class, bind it
                if hasattr(ClinicalAssertionModel, "predict_batch"):
                    original = ClinicalAssertionModel.predict_batch
    
                    async def bound_predict_batch(self, sentences):
                        return await original(model_instance, sentences)
    
                    model_instance.predict_batch = bound_predict_batch
    
            # Monkeypatch heavy internals if they exist to avoid external resources
            # For example, if model_instance has tokenizer or pipeline attributes, stub them
            for attr in ("tokenizer", "model", "pipeline", "nlp"):
                if not hasattr(model_instance, attr):
                    setattr(model_instance, attr, Mock())
    
            # If predict_batch exists and is coroutine, call it directly
            if hasattr(model_instance, "predict_batch") and inspect.iscoroutinefunction(
                model_instance.predict_batch
            ):
                try:
>                   result = await model_instance.predict_batch(input_sentences)

tests/generated/test_unit_20251113_135905_01.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <app.model.ClinicalAssertionModel object at 0x778a0e59e0f0>
sentences = ['Test sentence.']

    async def predict_batch(self, sentences: List[str]) -> List[Dict[str, Any]]:
        """Predict assertion status for multiple sentences"""
        if not self.is_loaded():
>           raise RuntimeError("Model is not loaded")
E           RuntimeError: Model is not loaded

../../test-repos/clinic/app/model.py:109: RuntimeError

During handling of the above exception, another exception occurred:

input_sentences = ['Test sentence.'], expect_exception = False
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x778a0e5a0560>

    @pytest.mark.asyncio
    @pytest.mark.parametrize(
        "input_sentences,expect_exception",
        [
            ([], False),  # empty batch - some implementations return empty list
            (["Test sentence."], False),  # normal path
            (["", None], True),  # invalid content may cause error
        ],
    )
    async def test_predict_batch_behavior(input_sentences, expect_exception, monkeypatch):
        """UNIVERSAL test for maximum coverage."""
        """
        Exercise ClinicalAssertionModel.predict_batch (or module-level predict_batch).
        The goal is to hit various internal branches: empty input, normal input, invalid input.
        If heavy model loading would occur, we attempt to instantiate and then monkeypatch
        model internals minimally to avoid external dependencies.
        """
        model_mod = safe_import("app.model")
    
        # Determine if there's a top-level function predict_batch
        target_func = None
        if hasattr(model_mod, "predict_batch") and inspect.iscoroutinefunction(
            getattr(model_mod, "predict_batch")
        ):
            target_func = getattr(model_mod, "predict_batch")
    
        ClinicalAssertionModel = getattr(model_mod, "ClinicalAssertionModel", None)
    
        # If there's a class method, prefer calling that to hit uncovered lines which often live there.
        if ClinicalAssertionModel is None and target_func is None:
            pytest.skip("No predict_batch function or ClinicalAssertionModel available in app.model")
    
        # Prepare a minimal model instance if needed
        if ClinicalAssertionModel:
            # Create an instance without running heavy __init__ if possible
            try:
                model_instance = ClinicalAssertionModel()
            except Exception:
                # If constructor is heavy, create a simple object and attach the original method
                model_instance = SimpleNamespace()
                # If a method exists on the class, bind it
                if hasattr(ClinicalAssertionModel, "predict_batch"):
                    original = ClinicalAssertionModel.predict_batch
    
                    async def bound_predict_batch(self, sentences):
                        return await original(model_instance, sentences)
    
                    model_instance.predict_batch = bound_predict_batch
    
            # Monkeypatch heavy internals if they exist to avoid external resources
            # For example, if model_instance has tokenizer or pipeline attributes, stub them
            for attr in ("tokenizer", "model", "pipeline", "nlp"):
                if not hasattr(model_instance, attr):
                    setattr(model_instance, attr, Mock())
    
            # If predict_batch exists and is coroutine, call it directly
            if hasattr(model_instance, "predict_batch") and inspect.iscoroutinefunction(
                model_instance.predict_batch
            ):
                try:
                    result = await model_instance.predict_batch(input_sentences)
                    if expect_exception:
                        # We expected an exception but got a result
                        assert False, f"Expected exception but got result: {result}"
                    else:
                        # Validate result format if possible - expect a list of dict-like results
                        assert isinstance(result, list)
                except Exception:
                    if not expect_exception:
>                       pytest.fail("predict_batch raised unexpectedly")
E                       Failed: predict_batch raised unexpectedly

tests/generated/test_unit_20251113_135905_01.py:209: Failed
_____________ TestVerifyAPIKey.test_verify_api_key_health_endpoint _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad858f260>
auth_client = <starlette.testclient.TestClient object at 0x778a0e59da60>

    def test_verify_api_key_health_endpoint(self, auth_client):
        """Test that health endpoint doesn't require API key"""
        response = auth_client.get("/health")
>       assert response.status_code == 200
E       assert 503 == 200
E        +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:55: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    app.main:main.py:229 Health check failed:
____________ TestVerifyAPIKey.test_verify_api_key_no_keys_required _____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad858fc80>
auth_client = <starlette.testclient.TestClient object at 0x778a0cdf9160>

    def test_verify_api_key_no_keys_required(self, auth_client):
        """Test endpoints when no API keys are required"""
        with patch.dict(os.environ, {"API_KEYS": "", "REQUIRE_API_KEY": "false"}):
            response = auth_client.get("/model/info")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:71: AssertionError
__________ TestVerifyAPIKey.test_verify_api_key_missing_from_request ___________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad85a4380>
auth_client = <starlette.testclient.TestClient object at 0x778a0cd612b0>

    def test_verify_api_key_missing_from_request(self, auth_client):
        """Test API key verification when key is missing"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info")
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:79: AssertionError
_________________ TestVerifyAPIKey.test_verify_api_key_invalid _________________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad85a46e0>
auth_client = <starlette.testclient.TestClient object at 0x778a0cd61670>

    def test_verify_api_key_invalid(self, auth_client):
        """Test API key verification with invalid key"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer invalid_key"}
            )
>           assert response.status_code == 401
E           assert 503 == 401
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:91: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_bearer _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad85a4a40>
auth_client = <starlette.testclient.TestClient object at 0x778a0ce21f40>

    def test_verify_api_key_valid_bearer(self, auth_client):
        """Test API key verification with valid Bearer token"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get(
                "/model/info", headers={"Authorization": "Bearer test_key"}
            )
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:103: AssertionError
______________ TestVerifyAPIKey.test_verify_api_key_valid_header _______________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad85a4da0>
auth_client = <starlette.testclient.TestClient object at 0x778a0cd61580>

    def test_verify_api_key_valid_header(self, auth_client):
        """Test API key verification with valid X-API-Key header"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info", headers={"X-API-Key": "test_key"})
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:111: AssertionError
____________ TestVerifyAPIKey.test_verify_api_key_valid_query_param ____________

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad85a5100>
auth_client = <starlette.testclient.TestClient object at 0x778a0d066e70>

    def test_verify_api_key_valid_query_param(self, auth_client):
        """Test API key verification with valid query parameter"""
        with patch.dict(
            os.environ, {"API_KEYS": "test_key", "REQUIRE_API_KEY": "true"}
        ):
            response = auth_client.get("/model/info?api_key=test_key")
>           assert response.status_code == 200
E           assert 503 == 200
E            +  where 503 = <Response [503 Service Unavailable]>.status_code

tests/manual/test_auth.py:119: AssertionError
___________ TestVerifyAPIKey.test_verify_api_key_logging_invalid_key ___________

self = <MagicMock name='logger.warning' id='131434804980544'>

    def assert_called(self):
        """assert that the mock was called at least once
        """
        if self.call_count == 0:
            msg = ("Expected '%s' to have been called." %
                   (self._mock_name or 'mock'))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to have been called.

/usr/lib/python3.12/unittest/mock.py:913: AssertionError

During handling of the above exception, another exception occurred:

self = <manual.test_auth.TestVerifyAPIKey object at 0x778ad85a5460>

    def test_verify_api_key_logging_invalid_key(self):
        """Test that invalid API key attempts are logged"""
        # Standard library imports
        from unittest.mock import Mock
    
        from app.auth import verify_api_key
    
        with patch.dict(
            os.environ, {"API_KEYS": "valid_key", "REQUIRE_API_KEY": "true"}
        ), patch("app.auth.logger") as mock_logger:
            mock_request = Mock()
            mock_request.url.path = "/api/test"
            mock_request.client = Mock()
            mock_request.client.host = "192.168.1.1"
            mock_request.headers = {}
            mock_request.query_params = {}
    
            # This should raise an exception and log the invalid attempt
            try:
                verify_api_key(mock_request, None)
            except Exception:
                pass
    
            # Verify that warning was logged for invalid key attempt
>           mock_logger.warning.assert_called()
E           AssertionError: Expected 'warning' to have been called.

tests/manual/test_auth.py:145: AssertionError
__________ TestRequestLoggingMiddleware.test_dispatch_with_exception ___________

request = <Mock id='131434808421952'>

    async def failing_call_next(request):
>       raise Exception("Test error")
E       Exception: Test error

tests/manual/test_middleware.py:248: Exception

During handling of the above exception, another exception occurred:

self = <app.middleware.RequestLoggingMiddleware object at 0x778a0d112db0>
request = <Mock id='131434808421952'>
call_next = <function TestRequestLoggingMiddleware.test_dispatch_with_exception.<locals>.failing_call_next at 0x778a0d0edb20>

    async def dispatch(
        self, request: Request, call_next: Callable[[Request], Awaitable[Response]]
    ) -> Response:
        start_time = time.time()
        request_id = request.headers.get(
            "X-Request-ID", f"req-{int(start_time * 1000)}"
        )
    
        logger.info(
            json.dumps(
                {
                    "event": "request_started",
                    "request_id": request_id,
                    "method": request.method,
                    "path": request.url.path,
                    "client_ip": self.get_client_id(request),
                    "timestamp": start_time,
                }
            )
        )
    
        try:
            response = await call_next(request)
    
            duration = time.time() - start_time
            logger.info(
                json.dumps(
                    {
                        "event": "request_completed",
                        "request_id": request_id,
                        "status_code": response.status_code,
                        "duration_ms": duration * 1000,
                        "timestamp": time.time(),
                    }
                )
            )
    
            response.headers["X-Request-ID"] = request_id
            return response
    
        except Exception as e:
            duration = time.time() - start_time
            logger.error(
                json.dumps(
                    {
                        "event": "request_failed",
                        "request_id": request_id,
                        "error": str(e),
                        "duration_ms": duration * 1000,
>                       "timestamp": time.time(),
                    }
                )
            )

../../test-repos/clinic/app/middleware.py:155: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.12/unittest/mock.py:1134: in __call__
    return self._mock_call(*args, **kwargs)
/usr/lib/python3.12/unittest/mock.py:1138: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='time' id='131434808469328'>, args = (), kwargs = {}
effect = <list_iterator object at 0x778a0d11f400>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
>               result = next(effect)
E               StopIteration

/usr/lib/python3.12/unittest/mock.py:1195: StopIteration

The above exception was the direct cause of the following exception:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x778ad85ca930>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x778a0d112db0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
            with pytest.raises(Exception, match="Test error"):
>               await middleware.dispatch(mock_request, failing_call_next)
E               RuntimeError: coroutine raised StopIteration

tests/manual/test_middleware.py:254: RuntimeError

During handling of the above exception, another exception occurred:

self = <manual.test_middleware.TestRequestLoggingMiddleware object at 0x778ad85ca930>
middleware = <app.middleware.RequestLoggingMiddleware object at 0x778a0d112db0>

    @pytest.mark.asyncio
    async def test_dispatch_with_exception(self, middleware):
        """Test request dispatch with exception"""
        mock_request = Mock()
        mock_request.method = "POST"
        mock_request.url.path = "/api/test"
        mock_request.headers = {}
        mock_request.client = None
    
        async def failing_call_next(request):
            raise Exception("Test error")
    
        with patch("time.time", side_effect=[1000.0, 1000.3]), patch(
            "app.middleware.logger"
        ) as mock_logger:
>           with pytest.raises(Exception, match="Test error"):
E           AssertionError: Regex pattern did not match.
E            Regex: 'Test error'
E            Input: 'coroutine raised StopIteration'

tests/manual/test_middleware.py:253: AssertionError
______________ TestClinicalAssertionModel.test_load_model_success ______________

self = <manual.test_model.TestClinicalAssertionModel object at 0x778a0e863140>
mock_cuda = <MagicMock name='is_available' id='131434808382560'>
mock_pipeline = <MagicMock name='TextClassificationPipeline' id='131434808459232'>
mock_model_class = <MagicMock name='from_pretrained' id='131434807772080'>
mock_tokenizer = <MagicMock name='from_pretrained' id='131434807775872'>
model = <app.model.ClinicalAssertionModel object at 0x778a0d109670>

    @pytest.mark.asyncio
    @patch("app.model.AutoTokenizer.from_pretrained")
    @patch("app.model.AutoModelForSequenceClassification.from_pretrained")
    @patch("app.model.TextClassificationPipeline")
    @patch("torch.cuda.is_available")
    async def test_load_model_success(
        self, mock_cuda, mock_pipeline, mock_model_class, mock_tokenizer, model
    ):
        """Test successful model loading"""
        mock_cuda.return_value = False
    
        # Mock the model and tokenizer
        mock_model_instance = Mock()
        mock_model_class.return_value = mock_model_instance
        mock_tokenizer_instance = Mock()
        mock_tokenizer.return_value = mock_tokenizer_instance
        mock_pipeline_instance = Mock()
        mock_pipeline.return_value = mock_pipeline_instance
    
        await model.load_model()
    
        assert model.tokenizer == mock_tokenizer_instance
>       assert model.model == mock_model_instance
E       AssertionError: assert <Mock name='from_pretrained().to().to()' id='131434807780000'> == <Mock name='from_pretrained()' id='131434808382608'>
E        +  where <Mock name='from_pretrained().to().to()' id='131434807780000'> = <app.model.ClinicalAssertionModel object at 0x778a0d109670>.model

tests/manual/test_model.py:82: AssertionError
_______________ TestClinicalAssertionModel.test_predict_success ________________

self = <manual.test_model.TestClinicalAssertionModel object at 0x778a0e8845c0>
mock_loop = <MagicMock name='get_event_loop' id='131434807772512'>
model = <app.model.ClinicalAssertionModel object at 0x778a0d0746b0>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_success(self, mock_loop, model):
        """Test successful prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock the pipeline result
        mock_result = {"label": "LABEL_0", "score": 0.95}
        model.pipeline.return_value = [mock_result]
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_result)
    
        result = await model.predict("test sentence")
    
>       assert result == {"label": "PRESENT", "score": 0.95}
E       AssertionError: assert {'label': 'LA...'score': 0.95} == {'label': 'PR...'score': 0.95}
E         Omitting 1 identical items, use -vv to show
E         Differing items:
E         {'label': 'LABEL_0'} != {'label': 'PRESENT'}
E         Full diff:
E         - {'label': 'PRESENT', 'score': 0.95}
E         ?            ^^ ^^^^
E         + {'label': 'LABEL_0', 'score': 0.95}
E         ?            ^^^ ^^^

tests/manual/test_model.py:145: AssertionError
____________ TestClinicalAssertionModel.test_predict_batch_success _____________

self = <manual.test_model.TestClinicalAssertionModel object at 0x778a0e885520>
mock_loop = <MagicMock name='get_event_loop' id='131434807699760'>
model = <app.model.ClinicalAssertionModel object at 0x778a0d062cf0>

    @pytest.mark.asyncio
    @patch("asyncio.get_event_loop")
    async def test_predict_batch_success(self, mock_loop, model):
        """Test successful batch prediction"""
        # Setup model as loaded
        model._loaded = True
        model.model = Mock()
        model.pipeline = Mock()
    
        # Mock batch results
        mock_results = [
            [{"label": "LABEL_0", "score": 0.95}],
            [{"label": "LABEL_1", "score": 0.87}],
        ]
        model.pipeline.return_value = mock_results
    
        # Mock asyncio loop
        mock_loop_instance = Mock()
        mock_loop.return_value = mock_loop_instance
        mock_loop_instance.run_in_executor = AsyncMock(return_value=mock_results)
    
        result = await model.predict_batch(["sentence 1", "sentence 2"])
    
        expected = [
            {"label": "PRESENT", "score": 0.95},
            {"label": "ABSENT", "score": 0.87},
        ]
>       assert result == expected
E       AssertionError: assert [[{'label': '...core': 0.87}]] == [{'label': 'P...score': 0.87}]
E         At index 0 diff: [{'label': 'LABEL_0', 'score': 0.95}] != {'label': 'PRESENT', 'score': 0.95}
E         Full diff:
E         - [{'label': 'PRESENT', 'score': 0.95}, {'label': 'ABSENT', 'score': 0.87}]
E         ?             ^^ ^^^^                                - ^^
E         + [[{'label': 'LABEL_0', 'score': 0.95}], [{'label': 'LABEL_1', 'score': 0.87}]]
E         ? +            ^^^ ^^^                 +  +           +   ^^^                  +

tests/manual/test_model.py:237: AssertionError
=============================== warnings summary ===============================
venv/lib/python3.12/site-packages/transformers/utils/generic.py:441
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

venv/lib/python3.12/site-packages/transformers/utils/generic.py:309
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
    _torch_pytree._register_pytree_node(

tests/generated/test_integ_20251113_135905_01.py::test_request_triggers_middleware_dispatch_and_logging
  /usr/lib/python3.12/unittest/mock.py:1292: RuntimeWarning: coroutine 'verify_api_key' was never awaited
    def __init__(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/generated/test_unit_20251113_135905_01.py::test_validate_sentence_via_model[Normal clinical sentence.-True]
  /usr/lib/python3.12/unittest/mock.py:1292: RuntimeWarning: coroutine 'ClinicalAssertionModel.load_model' was never awaited
    def __init__(
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/manual/test_api.py::TestHealthEndpoint::test_health_check_success
  /home/sigmoid/TECH_DEMO/new-tech-demo/venv/lib/python3.12/site-packages/django/db/models/manager.py:21: RuntimeWarning: coroutine 'general_exception_handler' was never awaited
    def __new__(cls, *args, **kwargs):
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
  /home/sigmoid/TECH_DEMO/new-tech-demo/tests/manual/test_auth.py:140: RuntimeWarning: coroutine 'verify_api_key' was never awaited
    verify_api_key(mock_request, None)
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform linux, python 3.12.3-final-0 -----------
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           154     24     16      2    84%   73-100, 160, 358, 432-437
/home/sigmoid/test-repos/clinic/app/middleware.py      92      0     14      0   100%
/home/sigmoid/test-repos/clinic/app/model.py           82      3     14      1    96%   45->48, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      3     10      3    92%   37, 86, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49      5     12      3    87%   26-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 487     50     84     11    87%
Coverage HTML written to dir htmlcov
Coverage XML written to file coverage.xml

=========================== short test summary info ============================
FAILED tests/generated/test_e2e_20251113_135905_01.py::test_health_check_model_not_loaded
FAILED tests/generated/test_e2e_20251113_135905_01.py::test_model_info_when_loaded
FAILED tests/generated/test_e2e_20251113_135905_01.py::test_clinical_model_predict_batch_direct
FAILED tests/generated/test_e2e_20251113_135905_01.py::test_middleware_dispatch_error_path
FAILED tests/generated/test_integ_20251113_135905_01.py::test_health_check_when_model_not_loaded_triggers_http_exception_handler
FAILED tests/generated/test_integ_20251113_135905_01.py::test_model_info_endpoint_success_returns_model_info
FAILED tests/generated/test_integ_20251113_135905_01.py::test_http_exception_handler_direct_invocation_increments_metrics
FAILED tests/generated/test_integ_20251113_135905_01.py::test_general_exception_handler_direct_invocation_produces_500_json
FAILED tests/generated/test_integ_20251113_135905_01.py::test_utils_sanitize_and_apply_hybrid_pipeline_edge_cases
FAILED tests/generated/test_unit_20251113_135905_01.py::test_predict_batch_behavior[input_sentences0-False]
FAILED tests/generated/test_unit_20251113_135905_01.py::test_predict_batch_behavior[input_sentences1-False]
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_health_endpoint
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_no_keys_required
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_missing_from_request
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_invalid
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_bearer
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_header
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_valid_query_param
FAILED tests/manual/test_auth.py::TestVerifyAPIKey::test_verify_api_key_logging_invalid_key
FAILED tests/manual/test_middleware.py::TestRequestLoggingMiddleware::test_dispatch_with_exception
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_load_model_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_success
FAILED tests/manual/test_model.py::TestClinicalAssertionModel::test_predict_batch_success
=========== 23 failed, 109 passed, 10 skipped, 6 warnings in 18.42s ============

ðŸ“Š Combined Coverage Analysis:
Name                                                Stmts   Miss Branch BrPart  Cover   Missing
-----------------------------------------------------------------------------------------------
/home/sigmoid/test-repos/clinic/app/__init__.py         1      0      0      0   100%
/home/sigmoid/test-repos/clinic/app/auth.py            39     15     18      2    53%   42, 47-70
/home/sigmoid/test-repos/clinic/app/main.py           157     26     18      3    82%   73-100, 160, 358, 432-437, 558-559
/home/sigmoid/test-repos/clinic/app/middleware.py      92      0     14      0   100%
/home/sigmoid/test-repos/clinic/app/model.py           82      3     14      1    96%   45->48, 117-119
/home/sigmoid/test-repos/clinic/app/schemas.py         70      3     10      3    92%   37, 86, 94
/home/sigmoid/test-repos/clinic/app/utils.py           49      5     12      3    87%   26-28, 39, 55, 104->108
-----------------------------------------------------------------------------------------------
TOTAL                                                 490     52     86     12    87%

= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
FINAL RESULTS
= =4.2 backend_code.log clinic.log coverage_gaps.json coverage.xml feature.log flask-high-coverage-repo.log gap_based.log htmlcov local_artifacts local_pipeline-1.sh manual_test_result.json output_combined.log output.log PIPELINE_SUMMARY.md pytest.ini QUALITY_GATE_SUMMARY.md requirements.txt src test.db tests venv 80
âœ… Manual Test Coverage:   85.01%
âœ… Combined Coverage:      89.73%
ðŸ“ˆ Coverage Improvement:   4.72%

âš ï¸  Quality Gate: Coverage 89.73% < 90%
ðŸ’¡ Consider:
   1. Review uncovered code in htmlcov/index.html
   2. Add more manual tests for complex scenarios
   3. Re-run gap analysis for another iteration

âœ… Pipeline completed with coverage improvement
