"""
Professional test file writer with enhanced formatting and metadata management.
"""
import datetime
import hashlib
import json
import pathlib
import re
import shutil
import ast
import textwrap
import traceback
from typing import Any, Dict, List, Set

# Professional header for generated test files
PROFESSIONAL_HEADER = '''"""generated by automated test generation system.

This module contains comprehensive tests following industry best practices:
- Arrange-Act-Assert pattern for clear test structure
- Comprehensive edge case and error condition coverage
- Proper mocking of external dependencies
- Deterministic test execution with no side effects
- Clear documentation and meaningful test names

Generated on: {timestamp}
Test framework: pytest
Coverage target: Comprehensive functional and integration testing
"""

'''

def _normalize_minimal(s: str) -> str:
    """Keep code shape while normalizing whitespace that breaks parsing."""
    return (s.replace("\r\n", "\n")
             .replace("\r", "\n")
             .replace("\t", "    ")
             .replace("\u00A0", " "))

def _fix_indentation_errors(code: str) -> str:
    """Fix specific indentation errors in generated code."""
    lines = code.splitlines()
    fixed_lines = []
    i = 0
    
    while i < len(lines):
        line = lines[i]
        fixed_lines.append(line)
        
        # Check if this line ends with colon (indicating a block)
        if line.rstrip().endswith(':') and not line.strip().startswith('#'):
            # Look ahead to see if next line is properly indented
            if i + 1 < len(lines):
                next_line = lines[i + 1]
                # If next line is not empty and not indented, fix it
                if (next_line.strip() and 
                    not next_line.startswith(' ') and 
                    not next_line.startswith('\t') and
                    not next_line.strip().startswith('#')):
                    # Insert proper indentation
                    indent = len(line) - len(line.lstrip())
                    fixed_lines.append(' ' * (indent + 4) + 'pass  # Added to fix indentation')
        
        i += 1
    
    return '\n'.join(fixed_lines)

def _ensure_all_blocks_have_content(code: str) -> str:
    """Ensure all code blocks (if, for, while, def, class, etc.) have content."""
    lines = code.splitlines()
    fixed_lines = []
    
    for i, line in enumerate(lines):
        fixed_lines.append(line)
        
        # Check if this is a block starter
        if (line.rstrip().endswith(':') and 
            not line.strip().startswith('#') and
            any(keyword in line for keyword in ['if ', 'for ', 'while ', 'def ', 'class ', 'with ', 'try:', 'except', 'finally:', 'else:'])):
            
            # Check if next line exists and is properly indented
            has_content = False
            if i + 1 < len(lines):
                next_line = lines[i + 1]
                current_indent = len(line) - len(line.lstrip())
                next_indent = len(next_line) - len(next_line.lstrip())
                
                # Next line should be indented more than current line
                if next_indent > current_indent and next_line.strip():
                    has_content = True
            
            # If no content, add a pass statement
            if not has_content:
                indent = len(line) - len(line.lstrip())
                fixed_lines.append(' ' * (indent + 4) + 'pass')
    
    return '\n'.join(fixed_lines)

def _validate_and_fix_syntax(code: str, file_path: pathlib.Path) -> str:
    """Validate and fix syntax errors in generated code."""
    # First try basic parsing
    try:
        ast.parse(code)
        return code
    except SyntaxError as e:
        print(f"Syntax error in generated code for {file_path}: {e}")
        
    # Try fixing indentation errors
    try:
        fixed_code = _fix_indentation_errors(code)
        fixed_code = _ensure_all_blocks_have_content(fixed_code)
        
        # Validate the fixed content
        try:
            ast.parse(fixed_code)
            print(f"Fixed syntax errors in {file_path}")
            return fixed_code
        except SyntaxError as e2:
            print(f"Could not fix syntax errors in {file_path}: {e2}")
    except Exception as fix_error:
        print(f"Error during syntax fixing for {file_path}: {fix_error}")
    
    # Try dedenting as last resort
    try:
        dedented = textwrap.dedent(code)
        ast.parse(dedented)
        print(f"Fixed syntax errors with dedent in {file_path}")
        return dedented
    except SyntaxError:
        pass
    
    # Final fallback: add warning comment but keep original
    print(f"All syntax fixes failed for {file_path}, using original with warning")
    return f"# WARNING: This file may contain syntax errors\n# Generation system could not fix all issues\n\n{code}"

def write_text(file_path: pathlib.Path, content: str):
    """Write test content with comprehensive error handling and validation."""
    file_path.parent.mkdir(parents=True, exist_ok=True)

    # Decide if header is needed
    header_needed = not content.startswith('"""') and not content.startswith('# """')
    ts = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
    header = PROFESSIONAL_HEADER.format(timestamp=ts)

    # 1) Enhanced path: validate and fix syntax before writing
    try:
        # Clean the content first
        cleaned_content = _normalize_minimal(content)
        
        # Apply syntax validation and fixes
        validated_content = _validate_and_fix_syntax(cleaned_content, file_path)
        
        # Add header if needed
        if header_needed:
            final_content = header + validated_content
        else:
            final_content = validated_content
        
        # Final validation before writing
        try:
            ast.parse(final_content)
            file_path.write_text(final_content, encoding="utf-8", newline="\n")
            print(f"Successfully wrote: {file_path}")
            return
        except SyntaxError as final_error:
            # Last resort: try formatted cleaning
            formatted_content = clean_and_format_content(final_content)
            try:
                ast.parse(formatted_content)
                file_path.write_text(formatted_content, encoding="utf-8", newline="\n")
                print(f"Successfully wrote (formatted): {file_path}")
                return
            except SyntaxError:
                # Write with warning
                warning_content = f"# WARNING: Syntax errors present\n# Final error: {final_error}\n\n{final_content}"
                file_path.write_text(warning_content, encoding="utf-8", newline="\n")
                print(f"Wrote with syntax warnings: {file_path}")
                return
                
    except Exception as e:
        print(f"Failed to write {file_path}: {e}")
        traceback.print_exc()
        
        # Absolute last resort: write raw content
        try:
            if header_needed:
                raw_content = header + content
            else:
                raw_content = content
            file_path.write_text(raw_content, encoding="utf-8", newline="\n")
            print(f"Wrote raw content to {file_path} (may have syntax errors)")
        except Exception as e2:
            print(f"Complete failure writing {file_path}: {e2}")

def clean_and_format_content(content: str) -> str:
    """Clean and format test content for professional appearance."""
    # Normalize line endings and tabs early
    content = content.replace("\r\n", "\n").replace("\r", "\n").replace("\t", "    ").replace("\u00A0", " ")

    # Remove any AI-related comments that slipped through
    ai_patterns = [
        r"(?i)#.*?(ai|llm|gpt|generated by|chatgpt|artificial intelligence).*?\n",
        r"(?i)#.*?note:.*?(ai|language model).*?\n"
    ]
    for pattern in ai_patterns:
        content = re.sub(pattern, "", content)

    fixes = [
        (r"\n{4,}", "\n\n\n"),
        (r"\n(class |def |@pytest\.)", r"\n\n\1"),
        (r"(import [^\n]+)\n(from [^\n]+)", r"\1\n\n\2"),
        (r'("""\s*\n)(\s*def|\s*class|\s*@)', r'\1\n\2'),
        (r"[ \t]+$", "", re.MULTILINE),
    ]
    for pattern, replacement, *flags in fixes:
        flags = flags[0] if flags else 0
        content = re.sub(pattern, replacement, content, flags=flags)

    content = organize_imports(content)
    return content

def organize_imports(content: str) -> str:
    """Organize imports following PEP 8 guidelines."""
    lines = content.split('\n')

    docstring_end = 0
    quote_count = 0
    for i, line in enumerate(lines):
        if '"""' in line:
            quote_count += line.count('"""')
            if quote_count >= 2:
                docstring_end = i + 1
                break

    header_part = lines[:docstring_end]
    remaining_lines = lines[docstring_end:]

    import_lines, other_lines, in_import_section = [], [], True
    for line in remaining_lines:
        stripped = line.strip()
        if stripped.startswith(('import ', 'from ')) and in_import_section:
            import_lines.append(line)
        elif stripped and not stripped.startswith('#') and in_import_section:
            in_import_section = False
            other_lines.append(line)
        else:
            other_lines.append(line)

    stdlib_imports, third_party_imports, local_imports = [], [], []
    for imp_line in import_lines:
        if any(stdlib in imp_line for stdlib in ['import os', 'import sys', 'import re', 'import json', 'import pathlib', 'import datetime', 'from typing', 'from unittest']):
            stdlib_imports.append(imp_line)
        elif any(third_party in imp_line for third_party in ['import pytest', 'import fastapi', 'import flask', 'import django', 'import requests', 'import httpx']):
            third_party_imports.append(imp_line)
        else:
            local_imports.append(imp_line)

    organized_imports = []
    if stdlib_imports:
        organized_imports.extend(sorted(set(stdlib_imports)))
        organized_imports.append("")
    if third_party_imports:
        organized_imports.extend(sorted(set(third_party_imports)))
        organized_imports.append("")
    if local_imports:
        organized_imports.extend(sorted(set(local_imports)))
        organized_imports.append("")

    result_lines = header_part + organized_imports + other_lines
    return '\n'.join(result_lines)

def final_content_cleanup(content: str) -> str:
    """Final cleanup pass for professional test file formatting."""
    content = content.rstrip() + '\n'
    content = re.sub(r'\n{4,}', '\n\n\n', content)
    for section in ['class Test', 'def test_', '@pytest.']:
        content = re.sub(f'(\\n)({section})', r'\1\1\2', content)
    content = re.sub(r'(def [^:]+:)\n\n\n', r'\1\n', content)
    content = re.sub(r'(class [^:]+:)\n\n\n', r'\1\n', content)
    return content

def find_related_tests(output_dir: pathlib.Path, source_file: str) -> List[pathlib.Path]:
    """Find existing test files related to a source file for cleanup."""
    source_stem = pathlib.Path(source_file).stem
    source_module = source_file.replace('/', '.').replace('.py', '')
    related_tests = []
    for test_file in output_dir.rglob("test_*.py"):
        try:
            content = test_file.read_text(encoding="utf-8", errors="ignore")
            if any(ref in content for ref in [source_stem, source_module, f"target.{source_stem}", f"from {source_stem}", f"import {source_stem}"]):
                related_tests.append(test_file)
        except Exception:
            continue
    return related_tests

def cleanup_deleted_and_modified(output_dir: pathlib.Path, deleted_files: Set[str], modified_files: Set[str]):
    """Clean up test files for deleted and modified source files."""
    print(" Cleaning up obsolete test files...")
    cleanup_count = 0
    for deleted_file in deleted_files:
        for test_file in find_related_tests(output_dir, deleted_file):
            try:
                test_file.unlink()
                print(f" Removed test for deleted source: {test_file.name}")
                cleanup_count += 1
            except Exception as e:
                print(f" Could not remove {test_file}: {e}")
    for modified_file in modified_files:
        for test_file in find_related_tests(output_dir, modified_file):
            try:
                test_file.unlink()
                print(f" Removed outdated test for modified source: {test_file.name}")
                cleanup_count += 1
            except Exception as e:
                print(f" Could not remove {test_file}: {e}")
    cleanup_empty_directories(output_dir)
    print(f"Cleaned up {cleanup_count} obsolete test files" if cleanup_count else "No obsolete test files to clean up")

def cleanup_empty_directories(output_dir: pathlib.Path):
    """Remove empty directories from test output."""
    for directory in sorted(output_dir.rglob("*"), reverse=True):
        if directory.is_dir() and directory != output_dir:
            try:
                if not any(directory.iterdir()):
                    directory.rmdir()
                    print(f" Removed empty directory: {directory.name}")
            except OSError:
                continue

def update_manifest(output_dir: pathlib.Path, generated_files: List[str], change_summary: Dict[str, Any]):
    """Update test generation manifest with comprehensive metadata."""
    manifest_path = output_dir / "_manifest.json"
    if manifest_path.exists():
        try:
            with open(manifest_path, 'r', encoding='utf-8') as f:
                manifest_data = json.load(f)
        except Exception:
            manifest_data = {}
    else:
        manifest_data = {}

    if "runs" not in manifest_data:
        manifest_data["runs"] = []
    if "statistics" not in manifest_data:
        manifest_data["statistics"] = {}

    current_run = {
        "timestamp": datetime.datetime.utcnow().isoformat() + "Z",
        "files_generated": [pathlib.Path(f).name for f in generated_files],
        "file_count": len(generated_files),
        "change_summary": change_summary,
        "generation_metadata": {
            "total_size_bytes": sum(pathlib.Path(f).stat().st_size for f in generated_files if pathlib.Path(f).exists()),
            "average_file_size": None,
            "test_function_count": count_test_functions(generated_files),
        }
    }
    if current_run["file_count"] > 0:
        current_run["generation_metadata"]["average_file_size"] = (
            current_run["generation_metadata"]["total_size_bytes"] // current_run["file_count"]
        )

    manifest_data["runs"].append(current_run)
    manifest_data["statistics"].update({
        "total_runs": len(manifest_data["runs"]),
        "total_files_generated": sum(run["file_count"] for run in manifest_data["runs"]),
        "last_generation": current_run["timestamp"],
        "generation_summary": {
            "unit_tests": len([f for f in generated_files if "test_unit_" in f]),
            "integration_tests": len([f for f in generated_files if "test_integ_" in f]),
            "e2e_tests": len([f for f in generated_files if "test_e2e_" in f]),
        }
    })
    if len(manifest_data["runs"]) > 50:
        manifest_data["runs"] = manifest_data["runs"][-50:]

    try:
        manifest_path.parent.mkdir(parents=True, exist_ok=True)
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest_data, f, indent=2, ensure_ascii=False)
        print(f"Updated test manifest: {manifest_path}")
    except Exception as e:
        print(f" Could not update manifest: {e}")

def count_test_functions(file_paths: List[str]) -> int:
    """Count total test functions across generated files."""
    test_count = 0
    for file_path in file_paths:
        try:
            path = pathlib.Path(file_path)
            if path.exists():
                content = path.read_text(encoding="utf-8", errors="ignore")
                test_count += len(re.findall(r'^\s*def test_\w+', content, re.MULTILINE))
        except Exception:
            continue
    return test_count

def generate_test_summary(output_dir: pathlib.Path) -> str:
    """Generate a comprehensive summary of the test suite."""
    test_files = list(output_dir.glob("test_*.py"))
    if not test_files:
        return "No test files found in output directory."
    summary_data = {
        "total_files": len(test_files),
        "file_breakdown": {
            "unit": len([f for f in test_files if "test_unit_" in f.name]),
            "integration": len([f for f in test_files if "test_integ_" in f.name]),
            "e2e": len([f for f in test_files if "test_e2e_" in f.name]),
            "other": len([f for f in test_files if not any(t in f.name for t in ["test_unit_", "test_integ_", "test_e2e_"])])
        },
        "total_test_functions": count_test_functions([str(f) for f in test_files]),
        "total_size_kb": sum(f.stat().st_size for f in test_files) // 1024,
        "files": [f.name for f in sorted(test_files)]
    }
    summary_text = f"""
Test Suite Summary
==================

 Total Files: {summary_data['total_files']}
 Total Test Functions: {summary_data['total_test_functions']}
 Total Size: {summary_data['total_size_kb']} KB

File Breakdown:
- Unit Tests: {summary_data['file_breakdown']['unit']} files
- Integration Tests: {summary_data['file_breakdown']['integration']} files
- E2E Tests: {summary_data['file_breakdown']['e2e']} files
- Other Tests: {summary_data['file_breakdown']['other']} files

Generated Files:
"""
    for filename in summary_data['files']:
        summary_text += f"  - {filename}\n"
    return summary_text.strip()

def extract_python_only(text: str) -> str:
    if "```" in text:
        blocks = re.findall(r"```(?:python)?\s*(.*?)```", text, flags=re.IGNORECASE|re.DOTALL)
        if blocks:
            return "\n\n".join(blocks)
        return text.replace("```","")
    return text
def validate_code(code: str):
        if not code.strip(): return False, "empty output"
        if not re.search(r"def test_", code): return False, "no test functions"
        try: ast.parse(code); return True, ""
        except SyntaxError as e: return False, f"syntax error: {e}"
    
def massage(code: str): return code